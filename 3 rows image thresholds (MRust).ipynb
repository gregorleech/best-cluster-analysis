{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a533f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from numpy.fft import fft2, ifft2, fftshift\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import gaussian_filter1d as gf1d\n",
    "from scipy.ndimage import gaussian_filter as gf\n",
    "from scipy.ndimage import uniform_filter as uf\n",
    "from skimage.transform import downscale_local_mean #For binning\n",
    "\n",
    "from statistics import median\n",
    "from statistics import mode\n",
    "\n",
    "import xarray as xr #package for labeling and adding metadata to multi-dimensional arrays\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"../kai_colloids/PyDDM\") #must point to the PyDDM folder\n",
    "#import ddm_analysis_and_fitting as ddm   \n",
    "3\n",
    "import tiff_file \n",
    "\n",
    "import io \n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import glob #glob is helpful for searching for filenames or directories\n",
    "import pickle #for saving data\n",
    "### usually this block prints out \"nd2reader module not found. Reading of .nd2 files disabled.\" on the first run\n",
    "### this is fine (unless you need to read .nd2 files), just re-run this block to make the error go away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0792c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def floodfill_flat(array, x, y, rows, cols):\n",
    "    stack = []\n",
    "    if array[x,y] != -1:\n",
    "        return 0\n",
    "    stack.append((x,y))\n",
    "    currclustersize = 1\n",
    "    array[x,y] = -2\n",
    "    while stack:\n",
    "        xcurr, ycurr = stack.pop()\n",
    "        if xcurr > 0:\n",
    "            if array[xcurr-1,ycurr] == -1:\n",
    "                array[xcurr-1,ycurr] = -2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr-1,ycurr))\n",
    "        if xcurr < rows-1:\n",
    "            if array[xcurr+1,ycurr] == -1:\n",
    "                array[xcurr+1,ycurr] = -2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr+1,ycurr))\n",
    "        if ycurr > 0:\n",
    "            if array[xcurr,ycurr-1] == -1:\n",
    "                array[xcurr,ycurr-1] = -2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr,ycurr-1))\n",
    "        if ycurr < cols-1:\n",
    "            if array[xcurr,ycurr+1] == -1:\n",
    "                array[xcurr,ycurr+1] = -2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr,ycurr+1))\n",
    "    return currclustersize\n",
    "\n",
    "\n",
    "def markcluster_flat(array, x, y, rows, cols, clustersize):\n",
    "    stack = []\n",
    "    if array[x,y] != -2:\n",
    "        return\n",
    "    stack.append((x,y))\n",
    "    array[x,y] = clustersize\n",
    "    while stack:\n",
    "        xcurr, ycurr = stack.pop()\n",
    "        if xcurr > 0:\n",
    "            if array[xcurr-1,ycurr] == -2:\n",
    "                array[xcurr-1,ycurr] = clustersize\n",
    "                stack.append((xcurr-1,ycurr))\n",
    "        if xcurr < rows-1:\n",
    "            if array[xcurr+1,ycurr] == -2:\n",
    "                array[xcurr+1,ycurr] = clustersize\n",
    "                stack.append((xcurr+1,ycurr))\n",
    "        if ycurr > 0:\n",
    "            if array[xcurr,ycurr-1] == -2:\n",
    "                array[xcurr,ycurr-1] = clustersize\n",
    "                stack.append((xcurr,ycurr-1))\n",
    "        if ycurr < cols-1:\n",
    "            if array[xcurr,ycurr+1] == -2:\n",
    "                array[xcurr,ycurr+1] = clustersize\n",
    "                stack.append((xcurr,ycurr+1))\n",
    "                \n",
    "def rows_n_cols(im):\n",
    "    imarray = np.array(im)\n",
    "    rows, cols = np.shape(imarray)\n",
    "    return rows, cols\n",
    "\n",
    "### filtimage removes the background \n",
    "def filtimage(image, filtersize=600):\n",
    "    image = (image/100)**1.6 #(array)^1.6 to increase contrast (i think this works?)\n",
    "    image = (image*10) +1000\n",
    "    image = (image*1.0) - ((uf(image,filtersize))*1)  #(image) - unifrom-filtered(image) subtracts background\n",
    "    image = image + 1500  #return pixel array values back to their original range \n",
    "    return image\n",
    "\n",
    "def threshold_images(fsize, im, i, return_marked, filt):\n",
    "    if filt == False:\n",
    "        im = im\n",
    "    else:\n",
    "        im = filtimage(im, filtersize= fsize)\n",
    "    imarray = np.array(im)\n",
    "    rows, cols = np.shape(imarray)\n",
    "    sys.setrecursionlimit(rows*cols)\n",
    "    med = np.median(imarray)\n",
    "    if frame_key == 2:\n",
    "        if i == 5:\n",
    "            imarray = imarray + 0 #5 #20\n",
    "        if i == 6:\n",
    "            imarray = imarray + 0 #10 #60\n",
    "        if i == 7:\n",
    "            imarray = imarray + 0 #15 #105\n",
    "        if i > 7:\n",
    "            imarray = imarray + 0 #20 #125\n",
    "    thresh = 1.12*med  #1.8*med\n",
    "    arraythresh = (imarray) > thresh\n",
    "    threshimage = Image.fromarray(arraythresh)\n",
    "### mark unclustered pixels with -1\n",
    "    marked_arraythresh = -1*(imarray > thresh)\n",
    "    if return_marked == False:\n",
    "        return threshimage\n",
    "    if return_marked == True:\n",
    "        return marked_arraythresh\n",
    "    #return marked_arraythresh, threshimage\n",
    "\n",
    "def show_filtered_images(fsize, row, ax, i, frame_key, filt):\n",
    "    index_add = arr_length * (row -1)\n",
    "    if time_array[i] == 0:\n",
    "        empty_im = np.zeros((1440,1920))\n",
    "        ax.imshow(empty_im, cmap = 'gray')\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    elif filt == False:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        ax.imshow(raw_image, cmap = 'gray')\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\", fsize=none)\", fontsize=10)\n",
    "    else:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        filtered_image = filtimage(raw_image, filtersize= fsize)\n",
    "        ax.imshow(filtered_image, cmap = 'gray')\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\", fsize=\"+str(fsize)+\")\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)\n",
    "    \n",
    "def show_threshold_images(fsize, row, ax, i, frame_key, filt):\n",
    "    index_add = arr_length * (row -1)\n",
    "    return_marked = False\n",
    "    if time_array[i] == 0:\n",
    "        threshold_image = np.zeros((1440,1920))\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    else:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\") --> threshold\", fontsize=10)\n",
    "        threshold_image = threshold_images(fsize, raw_image, i, return_marked, filt)\n",
    "    ax.imshow(threshold_image, cmap = 'gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)\n",
    "    \n",
    "def cluster_analysis(size_lim, i, row_num, arraythresh, rows, cols):\n",
    "    #print('working...')\n",
    "    clusters = []\n",
    "    for x in range(rows):\n",
    "        #if x % 25 == 0:  #progress update\n",
    "        #    print(100*x/rows)\n",
    "        for y in range(cols):\n",
    "            if arraythresh[x,y] == -1:\n",
    "                #fill in this cluster\n",
    "                clustersize = floodfill_flat(arraythresh, x, y, rows, cols)\n",
    "                clusters.append(clustersize)\n",
    "                #print(clustersize,x,y)\n",
    "                #mark finished cluster with size\n",
    "                markcluster_flat(arraythresh,x,y,rows,cols,clustersize)\n",
    "                \n",
    "    for q in range(clusters.count(1)):\n",
    "        clusters.remove(1)\n",
    "    \n",
    "    clusters.sort()\n",
    "    if clusters[0] < size_lim:\n",
    "        keep_going = True\n",
    "        while keep_going:\n",
    "            if clusters[0] < size_lim:\n",
    "                m = clusters[0]\n",
    "                clusters.remove(m)\n",
    "            else:\n",
    "                keep_going = False\n",
    "                                                #max_size = (max(clusters)) * (pixel_size**2)  \n",
    "    total_num = len(clusters)\n",
    "    mean_size = (sum(clusters) / len(clusters)) * (pixel_size**2)  #pixel size squared b/c max size is an area\n",
    "    median_size = (median(clusters)) * (pixel_size**2)\n",
    "    mode_size = (mode(clusters)) * (pixel_size**2)\n",
    "\n",
    "    print(\"for row\"+str(row_num)+\", t= \"+str(time_array[i]) + \" hrs:\")\n",
    "    if (i == 0) and (row_num == 1):\n",
    "        print(clusters)\n",
    "    print('   clusters counted: %5i, mean size: %5.3f, median: %5.3f, mode: %5.3f' %(total_num, mean_size, \n",
    "                                                                                       median_size, mode_size))\n",
    "    return total_num, mean_size, median_size, mode_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ce838",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Z\"\n",
    "exp = \"9-5-22_s1_theBigOne\"\n",
    "### \"data_dir\" is the pathway to the folder holding the tiff files to be analyzed \n",
    "data_dir = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\all tiff files\\\\\"\n",
    "### \"plot_saveto\" is the pathway to the folder where plots and results will be saved\n",
    "saveto_dir = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\threshold results\\\\\"\n",
    "\n",
    "files = glob.glob(data_dir+\"*_t*\") ### this should generate an ordered list of files in \"data_dir\" which have \"_t\" in their name\n",
    "print(\"found %i files\" % len(files))\n",
    "for i,f in enumerate(files): print (' %i \\t %s' % (i, f.split('\\\\')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_names = [\"1-3 kA-WT\", \"WT (no kA)\", \"EA KaiC\", \"AE KaiC\"]\n",
    "### array containing the name for each frame in a tiff file to be run \n",
    "### e.g. frame 1 is an image of the \"50% bKaiB\" condition, frame 2 is an image of the \"35% bKaiB\" condition, etc.\n",
    "\n",
    "time_array = [0.7, 3.7, 6.7, 10.0, 12.8, 17.7, 21.7, 24.9, 28.2]\n",
    "#s1 [0.7, 3.7, 6.7, 10.0, 12.8, 17.7, 21.7, 24.9, 28.2]\n",
    "#s2 [1.0, 3.8, 6.8, 10.2, 13.1, 18.0, 21.9, 25.3, 28.7]\n",
    "### array containing the time points corresponding to consecutive tiff files\n",
    "### e.g. tiff files \"bottom_row_t1\", \"middle_row_t1\", and \"top_row_t1\" all correspond to t = 0.5 hrs, time_array[0]\n",
    "\n",
    "pixel_size = 0.091 \n",
    "### pixel size (microns per pixel) of frames/ images in the tiff files --- 40x olympus objective => 0.091 um/px\n",
    "### IF 2x2 BINNING: multiply the original pixel size by 2^2 = 4, e.g. 4*(0.091 um/px) = 0.364 um/px\n",
    "\n",
    "eg_im= tiff_file.imread(files[0])\n",
    "print(\"tiff file dimensions: \"+ str(eg_im.shape))\n",
    "\n",
    "arr_length = int((len(files))/3)\n",
    "print(\"total number of time points: \"+ str(arr_length))\n",
    "if arr_length % 2 == 0:\n",
    "    num_rows = int(arr_length/2)\n",
    "else:\n",
    "    num_rows = int((arr_length+1)/2)\n",
    "print(\"(for image previews) number of rows = \" + str(num_rows))\n",
    "\n",
    "font_size = 16\n",
    "### font size\n",
    "dpi_num = 800\n",
    "### image quality level (recommendation: 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef7ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_key = 2\n",
    "### \"frame_key\" specifies which frame of each tiff file will be analyzed (each frame of my tiff is for a different condition)\n",
    "### e.g. \"key = 0\" dictates that the first frame of each tiff file should be analyzed\n",
    "condition = frame_names[frame_key]\n",
    "print(\"condition: \"+condition)\n",
    "\n",
    "fsize = 1000\n",
    "filt = True\n",
    "### set filter size (pixel area used to estimate and remove average background pixel intensities, recommendation = 600\n",
    "if filt == False:\n",
    "    fil = \"none\" \n",
    "else:\n",
    "    fil = str(fsize)\n",
    "\n",
    "row = 3\n",
    "### choose which set of tiff files should be analyzed, row1, row2 or row3\n",
    "\n",
    "fig_height = num_rows*2.3\n",
    "time_array.append(0)\n",
    "print(time_array)\n",
    "i = 0\n",
    "fig, axs = plt.subplots(num_rows, 4, figsize=(10,fig_height))\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j % 2 == 0:\n",
    "        show_filtered_images(fsize, row, ax, i, frame_key, filt)\n",
    "    else:\n",
    "        show_threshold_images(fsize, row, ax, i, frame_key, filt)\n",
    "        i = i + 1 \n",
    "plt.show()\n",
    "time_array.remove(0)\n",
    "print(time_array)\n",
    "\n",
    "### option to save this figure (uncomment below)\n",
    "fig.savefig(saveto_dir+\"n threshold vs filtered images for \"+condition+\" (row\"+str(row)+\", f=\"+fil+\")\"+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "row1_thresh = [0.0] * int(arr_length)\n",
    "row2_thresh = [0.0] * int(arr_length)\n",
    "row3_thresh = [0.0] * int(arr_length)\n",
    "\n",
    "row1_results = [0.0] * int(arr_length)\n",
    "row2_results = [0.0] * int(arr_length)\n",
    "row3_results = [0.0] * int(arr_length)\n",
    "print(\"condition: \"+condition)\n",
    "print(\"fsize= \"+fil)\n",
    "\n",
    "beads_per_cluster = 5\n",
    "size_lim = int( (beads_per_cluster*0.785) / (pixel_size**2) ) + 2 #add 2 to round up just for good measure (i.e. 473+2=475)     \n",
    "### 2D bead area = pi*(0.5 um)^2 = 0.785 um^2\n",
    "# e.g (5 beads)(0.785 um^2) = 3.93 um^2 ---> (3.93 um^2)/((0.091 um/px)^2) = ~475 pixels^2\n",
    "print(\"define clusters as at least: \"+str(beads_per_cluster)+\" beads --> size_lim = \"+str(size_lim)+\" pixels^2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c66e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#frame_key = 0\n",
    "cmap = matplotlib.cm.get_cmap('Greens')\n",
    "\n",
    "return_marked = True\n",
    "for i in range(arr_length):\n",
    "    rows, cols, = rows_n_cols(tiff_file.imread(files[0], key=[frame_key]))\n",
    "    \n",
    "    row1_thresh[i] = threshold_images(fsize, tiff_file.imread(files[i],key=[frame_key]), i, return_marked, filt)\n",
    "    row2_thresh[i] = threshold_images(fsize, tiff_file.imread(files[i+arr_length], key=[frame_key]), i, return_marked, filt) \n",
    "    row3_thresh[i] = threshold_images(fsize, tiff_file.imread(files[i+(arr_length*2)],key=[frame_key]), i, return_marked, filt) \n",
    "    \n",
    "    r1_total_num, r1_mean_size, r1_median_size, r1_mode_size = cluster_analysis(size_lim, i, 1, row1_thresh[i], rows, cols)\n",
    "    r2_total_num, r2_mean_size, r2_median_size, r2_mode_size = cluster_analysis(size_lim, i, 2, row2_thresh[i], rows, cols)\n",
    "    r3_total_num, r3_mean_size, r3_median_size, r3_mode_size = cluster_analysis(size_lim, i, 3, row3_thresh[i], rows, cols)\n",
    "    \n",
    "    row1_results[i] = (r1_total_num, r1_mean_size, r1_median_size, r1_mode_size)\n",
    "    row2_results[i] = (r2_total_num, r2_mean_size, r2_median_size, r2_mode_size)\n",
    "    row3_results[i] = (r3_total_num, r3_mean_size, r3_median_size, r3_mode_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5025af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap_num = (arr_length*2) - 2\n",
    "fig_size = 9, 20   #20/1.618\n",
    "\n",
    "### set up empty arrays\n",
    "all_total_num = np.zeros((3,len(row1_results)))\n",
    "avg_total_num = np.empty(len(row1_results))\n",
    "stderror_total_num = np.empty(len(row1_results))\n",
    "\n",
    "### find average values and std. error \n",
    "for i in range(len(row1_results)):\n",
    "    all_total_num[0,i] = row1_results[i][0]\n",
    "    all_total_num[1,i] = row2_results[i][0]\n",
    "    all_total_num[2,i] = row3_results[i][0]\n",
    "avg_total_num = all_total_num.mean(axis=0)\n",
    "stderror_total_num = all_total_num.std(axis=0)/np.sqrt(3)\n",
    "\n",
    "### same process for the other output results\n",
    "all_mean_size = np.zeros((3,len(row1_results)))\n",
    "avg_mean_size = np.empty(len(row1_results))\n",
    "stderror_mean_size = np.empty(len(row1_results))\n",
    "for i in range(len(row1_results)):\n",
    "    all_mean_size[0,i] = row1_results[i][1]\n",
    "    all_mean_size[1,i] = row2_results[i][1]\n",
    "    all_mean_size[2,i] = row3_results[i][1]\n",
    "avg_mean_size = all_mean_size.mean(axis=0)\n",
    "stderror_mean_size = all_mean_size.std(axis=0)/np.sqrt(3)\n",
    "\n",
    "all_median_size = np.zeros((3,len(row1_results)))\n",
    "avg_median_size = np.empty(len(row1_results))\n",
    "stderror_median_size = np.empty(len(row1_results))\n",
    "for i in range(len(row1_results)):\n",
    "    all_median_size[0,i] = row1_results[i][2]\n",
    "    all_median_size[1,i] = row2_results[i][2]\n",
    "    all_median_size[2,i] = row3_results[i][2]\n",
    "avg_median_size = all_median_size.mean(axis=0)\n",
    "stderror_median_size = all_median_size.std(axis=0)/np.sqrt(3)\n",
    "\n",
    "all_mode_size = np.zeros((3,len(row1_results)))\n",
    "avg_mode_size = np.empty(len(row1_results))\n",
    "stderror_mode_size = np.empty(len(row1_results))\n",
    "for i in range(len(row1_results)):\n",
    "    all_mode_size[0,i] = row1_results[i][3]\n",
    "    all_mode_size[1,i] = row2_results[i][3]\n",
    "    all_mode_size[2,i] = row3_results[i][3]\n",
    "avg_mode_size = all_mode_size.mean(axis=0)\n",
    "stderror_mode_size = all_mode_size.std(axis=0)/np.sqrt(3)\n",
    "\n",
    "### plot average values and std. error for L1 correlation lengths\n",
    "fig = plt.figure(figsize=(fig_size))\n",
    "gs = fig.add_gridspec(4, top=0.95, hspace=0.09)\n",
    "axs = gs.subplots(sharex=False, sharey=False)\n",
    "#fig, axs = plt.subplots(3, figsize=(fig_size))\n",
    "markerSize = 8\n",
    "title = condition +\" n results (clusters= at least \"+str(beads_per_cluster)+\" beads, filter= \"+fil+\")\"\n",
    "\n",
    "for i in range(int(len(time_array))):\n",
    "    axs[0].set_title(title, fontsize= (font_size))\n",
    "    axs[0].set( ylabel='clusters found (number)')\n",
    "    axs[0].set_ylim(0, 400)\n",
    "    axs[0].plot(time_array[i], avg_total_num[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"max cluster size\")\n",
    "    axs[0].errorbar(time_array[i], avg_total_num[i], yerr = stderror_total_num[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "    axs[1].set( ylabel='mean cluster size ($\\mu$m)^2')\n",
    "    axs[1].set_ylim(0, 60)\n",
    "    axs[1].plot(time_array[i], avg_mean_size[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"mean cluster size\")\n",
    "    axs[1].errorbar(time_array[i], avg_mean_size[i], yerr = stderror_mean_size[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "    axs[2].set( ylabel='median cluster size ($\\mu$m)^2')\n",
    "    axs[2].set_ylim(0, 60)\n",
    "    axs[2].plot(time_array[i], avg_median_size[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"mean cluster size\")\n",
    "    axs[2].errorbar(time_array[i], avg_median_size[i], yerr = stderror_median_size[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "    axs[3].set( ylabel='mode cluster size ($\\mu$m)^2')\n",
    "    axs[3].set( xlabel=\"Time (hrs after adding KaiC)\")\n",
    "    axs[3].set_ylim(0, 30)\n",
    "    axs[3].plot(time_array[i], avg_mode_size[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"mean cluster size\")\n",
    "    axs[3].errorbar(time_array[i], avg_mode_size[i], yerr = stderror_mode_size[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(axis='both', which='major', labelsize=(font_size-3))\n",
    "    ax.xaxis.get_label().set_fontsize(font_size)\n",
    "    ax.yaxis.get_label().set_fontsize(font_size)\n",
    "\n",
    "#plt.subplot_tool()\n",
    "plt.show()\n",
    "fig.savefig(saveto_dir+title+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06876248",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_file = exp+\"-- results (clusters= at least \"+str(beads_per_cluster)+\" beads, filter= \"+fil+\")\"+\".csv\"\n",
    "data_file_exists = os.path.isfile(saveto_dir+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(\"Data file already exists.\")\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title])\n",
    "        writer.writerow(['clusters detected','','','','','',\n",
    "                         'max cluster size','','','','','',\n",
    "                         'mean cluster size','','','','','',\n",
    "                         'avg clustering of a pixel',''])\n",
    "        writer.writerow(['time (hrs)',\n",
    "                         'row1','row2','row3','avg','std error','',\n",
    "                         'row1','row2','row3','avg','std error','',\n",
    "                         'row1','row2','row3','avg','std error','',\n",
    "                         'row1','row2','row3','avg','std error'])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i], \n",
    "                             row1_results[i][0], row2_results[i][0], row3_results[i][0], \n",
    "                             avg_total_num[i], stderror_total_num[i], '',\n",
    "                             row1_results[i][1], row2_results[i][1], row3_results[i][1], \n",
    "                             avg_mean_size[i], stderror_mean_size[i], '',\n",
    "                             row1_results[i][2], row2_results[i][2], row3_results[i][2], \n",
    "                             avg_median_size[i], stderror_median_size[i], '', \n",
    "                             row1_results[i][3], row2_results[i][3], row3_results[i][3], \n",
    "                             avg_mode_size[i], stderror_mode_size[i]])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"Results appended to file.\")\n",
    "else:\n",
    "    print(\"Data file does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(saveto_dir+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title])\n",
    "        writer.writerow(['clusters detected','','','','','',\n",
    "                         'max cluster size','','','','','',\n",
    "                         'mean cluster size','','','','','',\n",
    "                         'avg clustering of a pixel',''])\n",
    "        writer.writerow(['time (hrs)',\n",
    "                         'row1','row2','row3','avg','std error','',\n",
    "                         'row1','row2','row3','avg','std error','',\n",
    "                         'row1','row2','row3','avg','std error','',\n",
    "                         'row1','row2','row3','avg','std error'])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i], \n",
    "                             row1_results[i][0], row2_results[i][0], row3_results[i][0], \n",
    "                             avg_total_num[i], stderror_total_num[i], '',\n",
    "                             row1_results[i][1], row2_results[i][1], row3_results[i][1], \n",
    "                             avg_mean_size[i], stderror_mean_size[i], '',\n",
    "                             row1_results[i][2], row2_results[i][2], row3_results[i][2], \n",
    "                             avg_median_size[i], stderror_median_size[i], '', \n",
    "                             row1_results[i][3], row2_results[i][3], row3_results[i][3], \n",
    "                             avg_mode_size[i], stderror_mode_size[i]])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc319f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_data_file = exp+\"--BP results (clusters= at least \"+str(beads_per_cluster)+\" beads, filter= \"+fil+\")\"+\".csv\"\n",
    "data_file_exists = os.path.isfile(saveto_dir+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(\"Data file already exists.\")\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['condition:','',title])\n",
    "        writer.writerow(['clusters detected','','','','','','','','','','','',\n",
    "                         'mean cluster size','','','','','','','','','','','',\n",
    "                         'median cluster size','','','','','','','','','','','',\n",
    "                         'mode cluster size','','','','','','','','','','','',])\n",
    "        writer.writerow(['time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',])\n",
    "        \n",
    "        writer.writerow(['row1',row1_results[1][0],row1_results[2][0],row1_results[3][0],row1_results[4][0],row1_results[5][0],\n",
    "                         row1_results[6][0],row1_results[7][0],row1_results[8][0],row1_results[i][0],'','',\n",
    "                         'row1',row1_results[1][1],row1_results[2][1],row1_results[3][1],row1_results[4][1],row1_results[5][1],\n",
    "                         row1_results[6][1],row1_results[7][1],row1_results[8][1],row1_results[i][1],'','',\n",
    "                         'row1',row1_results[1][2],row1_results[2][2],row1_results[3][2],row1_results[4][2],row1_results[5][2],\n",
    "                         row1_results[6][2],row1_results[7][2],row1_results[8][2],row1_results[i][2],'','',\n",
    "                         'row1',row1_results[1][3],row1_results[2][3],row1_results[3][3],row1_results[4][3],row1_results[5][3],\n",
    "                         row1_results[6][3],row1_results[7][3],row1_results[8][3],row1_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row2',row2_results[1][0],row2_results[2][0],row2_results[3][0],row2_results[4][0],row2_results[5][0],\n",
    "                         row2_results[6][0],row2_results[7][0],row2_results[8][0],row2_results[i][0],'','',\n",
    "                         'row2',row2_results[1][1],row2_results[2][1],row2_results[3][1],row2_results[4][1],row2_results[5][1],\n",
    "                         row2_results[6][1],row2_results[7][1],row2_results[8][1],row2_results[i][1],'','',\n",
    "                         'row2',row2_results[1][2],row2_results[2][2],row2_results[3][2],row2_results[4][2],row2_results[5][2],\n",
    "                         row2_results[6][2],row2_results[7][2],row2_results[8][2],row2_results[i][2],'','',\n",
    "                         'row2',row2_results[1][3],row2_results[2][3],row2_results[3][3],row2_results[4][3],row2_results[5][3],\n",
    "                         row2_results[6][3],row2_results[7][3],row2_results[8][3],row2_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row3',row3_results[1][0],row3_results[2][0],row3_results[3][0],row3_results[4][0],row3_results[5][0],\n",
    "                         row3_results[6][0],row3_results[7][0],row3_results[8][0],row3_results[i][0],'','',\n",
    "                         'row3',row3_results[1][1],row3_results[2][1],row3_results[3][1],row3_results[4][1],row3_results[5][1],\n",
    "                         row3_results[6][1],row3_results[7][1],row3_results[8][1],row3_results[i][1],'','',\n",
    "                         'row3',row3_results[1][2],row3_results[2][2],row3_results[3][2],row3_results[4][2],row3_results[5][2],\n",
    "                         row3_results[6][2],row3_results[7][2],row3_results[8][2],row3_results[i][2],'','',\n",
    "                         'row3',row3_results[1][3],row3_results[2][3],row3_results[3][3],row3_results[4][3],row3_results[5][3],\n",
    "                         row3_results[6][3],row3_results[7][3],row3_results[8][3],row3_results[i][3],'','',])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"Results appended to file.\")\n",
    "else:\n",
    "    print(\"Data file does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(saveto_dir+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['condition:','',title])\n",
    "        writer.writerow(['clusters detected','','','','','','','','','','','',\n",
    "                         'mean cluster size','','','','','','','','','','','',\n",
    "                         'median cluster size','','','','','','','','','','','',\n",
    "                         'mode cluster size','','','','','','','','','','','',])\n",
    "        writer.writerow(['time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',])\n",
    "        \n",
    "        writer.writerow(['row1',row1_results[1][0],row1_results[2][0],row1_results[3][0],row1_results[4][0],row1_results[5][0],\n",
    "                         row1_results[6][0],row1_results[7][0],row1_results[8][0],row1_results[i][0],'','',\n",
    "                         'row1',row1_results[1][1],row1_results[2][1],row1_results[3][1],row1_results[4][1],row1_results[5][1],\n",
    "                         row1_results[6][1],row1_results[7][1],row1_results[8][1],row1_results[i][1],'','',\n",
    "                         'row1',row1_results[1][2],row1_results[2][2],row1_results[3][2],row1_results[4][2],row1_results[5][2],\n",
    "                         row1_results[6][2],row1_results[7][2],row1_results[8][2],row1_results[i][2],'','',\n",
    "                         'row1',row1_results[1][3],row1_results[2][3],row1_results[3][3],row1_results[4][3],row1_results[5][3],\n",
    "                         row1_results[6][3],row1_results[7][3],row1_results[8][3],row1_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row2',row2_results[1][0],row2_results[2][0],row2_results[3][0],row2_results[4][0],row2_results[5][0],\n",
    "                         row2_results[6][0],row2_results[7][0],row2_results[8][0],row2_results[i][0],'','',\n",
    "                         'row2',row2_results[1][1],row2_results[2][1],row2_results[3][1],row2_results[4][1],row2_results[5][1],\n",
    "                         row2_results[6][1],row2_results[7][1],row2_results[8][1],row2_results[i][1],'','',\n",
    "                         'row2',row2_results[1][2],row2_results[2][2],row2_results[3][2],row2_results[4][2],row2_results[5][2],\n",
    "                         row2_results[6][2],row2_results[7][2],row2_results[8][2],row2_results[i][2],'','',\n",
    "                         'row2',row2_results[1][3],row2_results[2][3],row2_results[3][3],row2_results[4][3],row2_results[5][3],\n",
    "                         row2_results[6][3],row2_results[7][3],row2_results[8][3],row2_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row3',row3_results[1][0],row3_results[2][0],row3_results[3][0],row3_results[4][0],row3_results[5][0],\n",
    "                         row3_results[6][0],row3_results[7][0],row3_results[8][0],row3_results[i][0],'','',\n",
    "                         'row3',row3_results[1][1],row3_results[2][1],row3_results[3][1],row3_results[4][1],row3_results[5][1],\n",
    "                         row3_results[6][1],row3_results[7][1],row3_results[8][1],row3_results[i][1],'','',\n",
    "                         'row3',row3_results[1][2],row3_results[2][2],row3_results[3][2],row3_results[4][2],row3_results[5][2],\n",
    "                         row3_results[6][2],row3_results[7][2],row3_results[8][2],row3_results[i][2],'','',\n",
    "                         'row3',row3_results[1][3],row3_results[2][3],row3_results[3][3],row3_results[4][3],row3_results[5][3],\n",
    "                         row3_results[6][3],row3_results[7][3],row3_results[8][3],row3_results[i][3],'','',])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a99b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4f520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c10c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
