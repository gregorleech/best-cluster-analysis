{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from numpy.fft import fft2, ifft2, fftshift\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import gaussian_filter1d as gf1d\n",
    "from scipy.ndimage import gaussian_filter as gf\n",
    "from scipy.ndimage import uniform_filter as uf\n",
    "\n",
    "from skimage.transform import downscale_local_mean #For binning\n",
    "from skimage.filters import try_all_threshold\n",
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "import xarray as xr #package for labeling and adding metadata to multi-dimensional arrays\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"../kai_colloids/PyDDM\") #must point to the PyDDM folder\n",
    "#import ddm_analysis_and_fitting as ddm   \n",
    "\n",
    "import tiff_file \n",
    "\n",
    "import io \n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob #glob is helpful for searching for filenames or directories\n",
    "import pickle #for saving data\n",
    "### usually this block prints out \"nd2reader module not found. Reading of .nd2 files disabled.\" on the first run\n",
    "### this is fine (unless you need to read .nd2 files), just re-run this block to make the error go away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First define the functions we will need to use\n",
    "### The Structural Image Autocorrelation (SIA) function is the second function defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this function (newRadav) finds the radial average of the image autocorrelation in the SIA function \n",
    "def newRadav(im, limangles=False, angRange=None, mask=None, rev=False,\n",
    "             debug_q = None):\n",
    "    if mask is None:\n",
    "        hasMask = False\n",
    "    else:\n",
    "        hasMask = True\n",
    "    nx,ny = im.shape\n",
    "    xx = np.arange(-(nx-1)/2., nx/2.)\n",
    "    yy = np.arange(-(ny-1)/2., ny/2.)\n",
    "    #x,y = np.meshgrid(xx,yy)\n",
    "    x,y = np.meshgrid(yy,xx)\n",
    "    q = np.sqrt(x**2 + y**2)\n",
    "    angles = np.arctan2(x,y)\n",
    "    \n",
    "    qx = np.arange(-1*nx/2,nx/2)*(1./nx) * max(nx,ny)\n",
    "    qy = np.arange(-1*ny/2,ny/2)*(1./ny) * max(nx,ny)\n",
    "    qxx,qyy = np.meshgrid(qy,qx) #qy,qx is correct order\n",
    "    q_new = np.sqrt(qxx**2 + qyy**2)\n",
    "    \n",
    "    if debug_q is not None:\n",
    "        return q_new.round().astype(int)==debug_q\n",
    "    \n",
    "    if mask is None:\n",
    "        mask = np.ones_like(angles)\n",
    "    if angRange is not None:\n",
    "        w1 = np.where(angles>angRange[0])\n",
    "    else:\n",
    "        w1 = np.where(angles>(13*np.pi/14))\n",
    "    if mask is None:\n",
    "        mask[w1]=0\n",
    "        mask = mask * np.rot90(np.rot90(mask))\n",
    "        mask = mask * np.flipud(mask)\n",
    "        mask[np.where(mask==0)] = np.nan\n",
    "        if rev:\n",
    "            mask = np.rot90(mask)\n",
    "    qr = q_new.round().astype(int)\n",
    "    #rs = np.arange(0,(nx-1)/2)\n",
    "    rs = np.arange(0,(max(nx,ny)-1)/2) \n",
    "    radav = np.zeros((len(rs)),dtype=float)\n",
    "    for i in range(0,len(rs)):\n",
    "        w = np.where(qr==rs[i])\n",
    "        if len(w[0])>0:\n",
    "            if limangles or hasMask:\n",
    "                newim = im*mask\n",
    "                radav[i] = np.nanmean(newim[w])\n",
    "            else:\n",
    "                radav[i] = np.nanmean(im[w])\n",
    "        #else:\n",
    "        #    print i\n",
    "    return radav\n",
    "\n",
    "def SIA_threshold_images(im, filter, fsize):\n",
    "    if filter:\n",
    "        im = filtimage(im, filtersize= fsize)\n",
    "    else:\n",
    "        im = im\n",
    "    imarray = np.array(im)\n",
    "    rows, cols = np.shape(imarray)\n",
    "    sys.setrecursionlimit(rows*cols)\n",
    "    med = np.median(imarray)\n",
    "    thresh = 1.09*med  #1.8*med\n",
    "    if key == 2:\n",
    "        if i == 5:\n",
    "            imarray = imarray + 20\n",
    "        if i == 6:\n",
    "            imarray = imarray + 60\n",
    "        if i == 7:\n",
    "            imarray = imarray + 100\n",
    "        if i > 7:\n",
    "            imarray = imarray + 120\n",
    "    #print(thresh)\n",
    "    arraythresh = (imarray) > thresh\n",
    "    threshimage = Image.fromarray(arraythresh)\n",
    "    #mark unclustered pixels with -1\n",
    "    marked_arraythresh = -1*(imarray > thresh)\n",
    "    return marked_arraythresh\n",
    "    #return marked_arraythresh, threshimage\n",
    "\n",
    "### the SIA function \n",
    "def SIA(image, filter = True, fsize=600, bin=True, binsize=2):\n",
    "    ''' Computes image autocorrelation. \n",
    "    Takes as input:\n",
    "        image: 2D image\n",
    "        filter: Boolean, if true will filter image with uniform filter\n",
    "        filtersize: size for uniform filtering\n",
    "    Returns:\n",
    "        corr_im: the image autocorrelation (this will be same size as image)\n",
    "        rav_corr: radially averaged image autocorrelation '''\n",
    "    \n",
    "    ### Crop image\n",
    "    #image = image[:1440, :1440]    ###option to crop out any large noise features \n",
    "\n",
    "    image = SIA_threshold_images(image, filter, fsize)\n",
    "        #image = image*1.0 - uf(image,filtersize)   ###uniform filter, removes background. (\"filtersize\" is pixel area)\n",
    "    if bin:\n",
    "        image = downscale_local_mean(image, (binsize,binsize), cval=1)   ### OPTIONAL (binning makes things run faster)\n",
    "    image = 1.0*image-image.mean() #subtract mean\n",
    "    image = image/image.std() #normalize by standard deviation\n",
    "    corr_im = np.real(fftshift(ifft2(fft2(image)*np.conj(fft2(image)))))/(image.shape[0]*image.shape[1])\n",
    "    \n",
    "    ### radial average taken\n",
    "    rav_corr = newRadav(corr_im)\n",
    "    \n",
    "    ### return ONLY rav_corr **b/c I don't need corr_im** (change this if you do need corr_im) \n",
    "    return rav_corr\n",
    "\n",
    "###define the single exponential we use to fit SIA curves\n",
    "str_equation = False\n",
    "def single_exponential(x, A, cl):\n",
    "    #return (np.exp(-x/cl) + A) \n",
    "    if str_equation == True:\n",
    "        return \"(1-A)*exp(-x/cl) + A\"\n",
    "    else:\n",
    "        return (1-A)*(np.exp(-x/cl)) + A\n",
    "\n",
    "### define function to systematically reduce the fit length until the fit parameters meet the specific criteria\n",
    "###### this function was much more relevant for finicky double exponential fits, it doesn't do much for single exponential fits\n",
    "def check_fits(A, l1, y_array, retry_num):\n",
    "    if A == r1_A:\n",
    "        row = \"(r1) \"\n",
    "    elif A == r2_A:\n",
    "        row = \"(r2) \"\n",
    "    else:\n",
    "        row = \"(r3) \"\n",
    "    j=1\n",
    "    skip_to_next = 0\n",
    "    if (A < 0.0001) or (l1 > 180.0): ##this line sets the specific criteria for fit params to meet\n",
    "        try_again = True\n",
    "        print(\"     poor fit at \"+row+ time)\n",
    "        while try_again:\n",
    "            new_fit_lim = fit_lim - ((2*j))\n",
    "            x_fit_lim = all_xvalues[new_fit_lim]\n",
    "            try:\n",
    "                popt, pcov = curve_fit(single_exponential, all_xvalues[0:new_fit_lim], y_array[0:new_fit_lim], p0 = [0.5,7.0])\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError - curve_fit failed\")\n",
    "                skip_to_next = 1\n",
    "            print(\"        \"+row+time+\"-- xlim= \"+str(x_fit_lim)+', fit: A=%5.3f, l1=%5.3f' % tuple(popt))\n",
    "            A, l1 = tuple(popt)\n",
    "            x_fit_values = np.linspace(all_xvalues[1], all_xvalues[new_fit_lim], 1000)\n",
    "            if (j == (retry_num-1)) or ((np.abs(l1)) < 0.1):\n",
    "                print(\"   poor fit at \"+row+ time + \" --> try double exponential fit?\")\n",
    "                try_again = False\n",
    "                try_single = True\n",
    "                i = 0\n",
    "\n",
    "            elif (A < 0.049) or (skip_to_next == 1):\n",
    "                try_again = True\n",
    "                skip_to_next = 0\n",
    "                j = j+1\n",
    "            else:\n",
    "                try_again = False\n",
    "                print(\"  yay!! good fit at \"+row+time)\n",
    "                print(\"  \"+row+time+\"-- xlim= \"+str(x_fit_lim)+', fit: A=%5.3f, l1=%5.3f' % tuple(popt))\n",
    "    return [A, l1]\n",
    "    \n",
    "### filtimage removes the background \n",
    "def filtimage(image, filtersize):\n",
    "    #image = (image/100)**1.6 #(array)^1.6 to increase contrast (i think this works?)\n",
    "    #image = (image*10) +1000\n",
    "    image = (image)**1.6\n",
    "    image = (image*1.0) - ((uf(image,filtersize))*1)  #(image) - unifrom-filtered(image) subtracts background\n",
    "    #image = image + 1500  #return pixel array values back to their original range \n",
    "    return image\n",
    "\n",
    "### the following functions ('show_threshold_images', and 'show_filtered_images') are used to preview \n",
    "### the images intended for analysis\n",
    "def show_filtered_images(fsize, row, ax, i, frame_key, filt):\n",
    "    index_add = arr_length * (row -1)\n",
    "    \n",
    "    #print(files[i+index_add])\n",
    "    if time_array[i] == 0:\n",
    "        empty_im = np.zeros((1440,1920))\n",
    "        ax.imshow(empty_im, cmap = 'gray')\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    elif filt == False:\n",
    "        ax.imshow(raw_image, cmap = 'gray')\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\", fsize=none)\", fontsize=10)\n",
    "    else:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        filtered_image = filtimage(raw_image, filtersize= fsize)\n",
    "        final_image = downscale_local_mean(filtered_image, (2,2), cval=1)\n",
    "        ax.imshow(final_image, cmap = 'gray')\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\", fsize=\"+str(fsize)+\")\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)\n",
    "    \n",
    "def show_threshold_images(fsize, row, ax, i, frame_key, filt):\n",
    "    index_add = arr_length * (row -1)\n",
    "    return_marked = True\n",
    "    if time_array[i] == 0:\n",
    "        final_image = np.zeros((1440,1920))\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    else:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\") --> threshold\", fontsize=10)\n",
    "        threshold_image = threshold_images(fsize, raw_image, i, return_marked, filt)\n",
    "        image = downscale_local_mean(threshold_image, (2,2), cval=1)\n",
    "        final_image = -1*image\n",
    "        if i == 1:\n",
    "            np.set_printoptions(threshold=sys.maxsize)\n",
    "            print(final_image[0:100])\n",
    "    ax.imshow(final_image, cmap = 'gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)\n",
    "    \n",
    "def threshold_images(fsize, im, i, return_marked, filt):\n",
    "    if filt == False:\n",
    "        im = im\n",
    "    else:\n",
    "        im = filtimage(im, filtersize= fsize)\n",
    "    imarray = np.array(im)\n",
    "    rows, cols = np.shape(imarray)\n",
    "    sys.setrecursionlimit(rows*cols)\n",
    "    med = np.mean(imarray)\n",
    "    if frame_key == 2:\n",
    "        if i == 5:\n",
    "            imarray = imarray + 20\n",
    "        if i == 6:\n",
    "            imarray = imarray + 60\n",
    "        if i == 7:\n",
    "            imarray = imarray + 105\n",
    "        if i > 7:\n",
    "            imarray = imarray + 125\n",
    "    thresh = 1.3*med #1.12*med  #1.8*med\n",
    "    arraythresh = (imarray) > thresh\n",
    "    threshimage = Image.fromarray(arraythresh)\n",
    "### mark unclustered pixels with -1\n",
    "    marked_arraythresh = -1*(imarray > thresh)\n",
    "    \n",
    "    sorted_MAt = sorted(marked_arraythresh)\n",
    "    if clusters[0] < size_lim:\n",
    "        keep_going = True\n",
    "        while keep_going:\n",
    "            if clusters[0] < size_lim:\n",
    "                m = clusters[0]\n",
    "                clusters.remove(m)\n",
    "            else:\n",
    "                keep_going = False\n",
    "    \n",
    "    if return_marked == False:\n",
    "        return threshimage\n",
    "    if return_marked == True:\n",
    "        return marked_arraythresh\n",
    "    #return marked_arraythresh, threshimage\n",
    "    \n",
    "def find_median(lst):\n",
    "    sortedLst = sorted(lst)\n",
    "    lstLen = len(lst)\n",
    "    index = (lstLen - 1) // 2\n",
    "    if (lstLen % 2):\n",
    "        return sortedLst[index]\n",
    "    else:\n",
    "        return (sortedLst[index] + sortedLst[index + 1])/2.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate your data (tiff files) and choose where to save results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory = \"Z\"\n",
    "exp = \"10-15-22_s2_theBigOne2\"\n",
    "### \"data_dir\" is the pathway to the folder holding the tiff files to be analyzed \n",
    "data_dir = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\all tiff files\\\\\" \n",
    "data_dir2 = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\SIA preview images\\\\\" #bottom_row_t01\\\\\n",
    "\n",
    "### \"plot_saveto\" is the pathway to the folder where plots and results will be saved\n",
    "plot_saveto= directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\SIA\\\\Data\\\\\"+exp+\"\\\\\"\n",
    "\n",
    "files = glob.glob(data_dir+\"*_t*\") ### this should generate an ordered list of files in \"data_dir\" which have \"_t\" in their name\n",
    "print(\"found %i files\" % len(files))\n",
    "for i,f in enumerate(files): print (' %i \\t %s' % (i, f.split('\\\\')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify pixel size, each condition (frame_names), and the time points of data collection (time_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_names = [\"1-3 kA-WT\", \"WT (no kA)\", \"EA KaiC\", \"AE KaiC\"]\n",
    "### array containing the name for each frame in a tiff file to be run \n",
    "### e.g. frame 1 is an image of the \"50% bKaiB\" condition, frame 2 is an image of the \"35% bKaiB\" condition, etc.\n",
    "\n",
    "time_array = [1.2, 4.4, 7.8, 10.8, 15.2, 18.3, 20.8, 24.3, 28.0]\n",
    "#s1 [1.1, 4.0, 7.4, 10.3, 14.7, 17.7, 20.3, 23.8, 27.0]\n",
    "#s2 [1.2, 4.4, 7.8, 10.8, 15.2, 18.3, 20.8, 24.3, 28.0]\n",
    "### array containing the time points corresponding to consecutive tiff files\n",
    "### e.g. tiff files \"bottom_row_t1\", \"middle_row_t1\", and \"top_row_t1\" all correspond to t = 0.5 hrs, time_array[0]\n",
    "\n",
    "pixel_size = 0.364 # 4*0.091 = 0.364\n",
    "### pixel size (microns per pixel) of frames/ images in the tiff files --- 40x olympus objective => 0.091 um/px\n",
    "### IF 2x2 BINNING: multiply the original pixel size by 2^2 = 4, e.g. 4*(0.091 um/px) = 0.364 um/px\n",
    "\n",
    "eg_im= tiff_file.imread(files[0])\n",
    "print(\"tiff file dimensions: \"+ str(eg_im.shape))\n",
    "\n",
    "arr_length = int((len(files))/6)\n",
    "print(\"total number of time points: \"+ str(arr_length))\n",
    "if arr_length % 2 == 0:\n",
    "    num_rows = int(arr_length/2)\n",
    "else:\n",
    "    num_rows = int((arr_length+1)/2)\n",
    "print(\"(for image previews) number of rows = \" + str(num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose size, font, and quality level (dpi_num) for the plots to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = 10,10/1.618\n",
    "###  size of output figures\n",
    "font_size = 16\n",
    "### font size\n",
    "dpi_num = 600\n",
    "### image quality level (recommendation: 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "def filtimage(image, filtersize):\n",
    "    image = (image/100)**1.6 #(array)^1.6 to increase contrast (i think this works?)\n",
    "    image = (image*10) +1000\n",
    "    image = (image*1.0) - ((uf(image,filtersize))*1)  #(image) - unifrom-filtered(image) subtracts background\n",
    "    image = image + 1500  #return pixel array values back to their original range \n",
    "    return image\n",
    "\n",
    "raw_image = tiff_file.imread(files[38],key=[2])\n",
    "image = filtimage(raw_image, filtersize= 1000)\n",
    "\n",
    "Red_cmap = matplotlib.cm.get_cmap('Reds')\n",
    "Blue_cmap = matplotlib.cm.get_cmap('Blues')\n",
    "Green_cmap = matplotlib.cm.get_cmap('Greens')\n",
    "Grey_cmap = matplotlib.cm.get_cmap('Greys')\n",
    "\n",
    "axs[0].plot(time_array[i], means[0][i],'o', ms=markerSize, c=Red_cmap(0.9-(i/cmap_num)), label = frame_names[0])\n",
    "axs[0].plot(time_array[i], means[1][i],'o', ms=markerSize, c=Blue_cmap(0.9-(i/cmap_num)), label = frame_names[1])\n",
    "axs[0].plot(time_array[i], means[2][i],'o', ms=markerSize, c=Green_cmap(0.9-(i/cmap_num)), label = frame_names[2])\n",
    "axs[0].plot(time_array[i], means[3][i],'o', ms=markerSize, c=Grey_cmap(0.9-(i/cmap_num)), label = frame_names[3])\n",
    "#axs[0].legend()\n",
    "\n",
    "axs[1].set( ylabel='median intensity')\n",
    "#axs[1].set_ylim(0, 60)\n",
    "axs[1].plot(time_array[i], medians[0][i],'o', ms=markerSize, c=Red_cmap(0.9-(i/cmap_num)), label = frame_names[0])\n",
    "axs[1].plot(time_array[i], medians[1][i],'o', ms=markerSize, c=Blue_cmap(0.9-(i/cmap_num)), label = frame_names[1])\n",
    "axs[1].plot(time_array[i], medians[2][i],'o', ms=markerSize, c=Green_cmap(0.9-(i/cmap_num)), label = frame_names[2])\n",
    "axs[1].plot(time_array[i], medians[3][i],'o', ms=markerSize, c=Grey_cmap(0.9-(i/cmap_num)), label = frame_names[3])\n",
    "#axs[1].legend()\n",
    "\n",
    "axs[2].set( ylabel='mode intensity')\n",
    "#axs[2].set_ylim(0, 60)\n",
    "axs[2].plot(time_array[i], modes[0][i],'o', ms=markerSize, c=Red_cmap(0.9-(i/cmap_num)), label = frame_names[0])\n",
    "axs[2].plot(time_array[i], modes[1][i],'o', ms=markerSize, c=Blue_cmap(0.9-(i/cmap_num)), label = frame_names[1])\n",
    "axs[2].plot(time_array[i], modes[2][i],'o', ms=markerSize, c=Green_cmap(0.9-(i/cmap_num)), label = frame_names[2])\n",
    "axs[2].plot(time_array[i], modes[3][i],'o', ms=markerSize, c=Grey_cmap(0.9-(i/cmap_num)), label = frame_names[3])\n",
    "#axs[2].legend()\n",
    "\n",
    "axs[3].set( ylabel='std deviation')\n",
    "axs[3].set( xlabel=\"Time (hrs after adding KaiC)\")\n",
    "#axs[3].set_ylim(0, 30)\n",
    "axs[3].plot(time_array[i], std_devs[0][i],'s', ms=markerSize, c=Red_cmap(0.9-(i/cmap_num)), label = frame_names[0])\n",
    "axs[3].plot(time_array[i], std_devs[1][i],'s', ms=markerSize, c=Blue_cmap(0.9-(i/cmap_num)), label = frame_names[1])\n",
    "axs[3].plot(time_array[i], std_devs[2][i],'s', ms=markerSize, c=Green_cmap(0.9-(i/cmap_num)), label = frame_names[2])\n",
    "axs[3].plot(time_array[i], std_devs[3][i],'s', ms=markerSize, c=Grey_cmap(0.9-(i/cmap_num)), label = frame_names[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "def filtimage(image, filtersize):\n",
    "    image = (image/100)**1.6 #(array)^1.6 to increase contrast (i think this works?)\n",
    "    image = (image*10) +1000\n",
    "    image = (image*1.0) - ((uf(image,filtersize))*1)  #(image) - unifrom-filtered(image) subtracts background\n",
    "    image = image + 1500  #return pixel array values back to their original range \n",
    "    return image\n",
    "\n",
    "raw_image = tiff_file.imread(files[38],key=[2])\n",
    "image = filtimage(raw_image, filtersize= 1000)\n",
    "\n",
    "block_size = 401\n",
    "\n",
    "raw_global_thresh = threshold_otsu(raw_image)\n",
    "raw_binary_global = raw_image > raw_global_thresh\n",
    "raw_local_thresh = threshold_local(raw_image, block_size, offset= -50)\n",
    "raw_binary_local = raw_image > raw_local_thresh\n",
    "\n",
    "global_thresh = threshold_otsu(image)\n",
    "binary_global = image > global_thresh\n",
    "local_thresh = threshold_local(image, block_size, offset= -50)\n",
    "#binary_local = Image.fromarray((image > local_thresh))\n",
    "binary_local = 1*(image > local_thresh)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(9,5))\n",
    "#ax = axes.ravel()\n",
    "plt.gray()\n",
    "\n",
    "ax[0, 0].imshow(raw_image)\n",
    "ax[0, 0].set_title('Original')\n",
    "ax[0, 0].axis('off')\n",
    "ax[0, 1].imshow(raw_binary_global)\n",
    "ax[0, 1].set_title('Global thresholding')\n",
    "ax[0, 1].axis('off')\n",
    "ax[0, 2].imshow(raw_binary_local)\n",
    "ax[0, 2].set_title('Local thresholding')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "ax[1, 0].imshow(image)\n",
    "ax[1, 0].set_title('filtered')\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 1].imshow(binary_global)\n",
    "ax[1, 1].set_title('Global thresholding')\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 2].imshow(binary_local)\n",
    "ax[1, 2].set_title('Local thresholding')\n",
    "ax[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout(pad=.2)\n",
    "plt.show()\n",
    "\n",
    "figure, axes = plt.subplots(1, 3, figsize=(9,3.5))\n",
    "axes[0].hist(raw_image.ravel(), bins=256)\n",
    "axes[0].axvline(raw_global_thresh, color='r')\n",
    "axes[0].set_title('original image global threshold')\n",
    "axes[1].hist(image.ravel(), bins=256)\n",
    "axes[1].axvline(global_thresh, color='r')\n",
    "axes[1].set_title('filtered image global threshold')\n",
    "\n",
    "num_binary_local = np.zeros((1440,1920))\n",
    "\n",
    "axes[2].hist(binary_local.ravel(), bins=\"auto\")\n",
    "axes[2].set_title('local thresholded image')\n",
    "plt.tight_layout(pad=.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "block_size = 1051\n",
    "offset_val = -20\n",
    "\n",
    "image = tiff_file.imread(files[9],key=[0])\n",
    "thresh = threshold_local(image, block_size, offset= offset_val)\n",
    "binary_im = 1*(image > thresh)\n",
    "binned_binary_im = (downscale_local_mean(binary_im, (2,2), cval=1))\n",
    "\n",
    "ed_image = (image**0.2)*500\n",
    "ed_thresh = threshold_local(ed_image, block_size, offset= offset_val)\n",
    "ed_binary_im = 1*(ed_image > ed_thresh)\n",
    "\n",
    "offset_binned_im = 1*((image*0.98) > (thresh+0))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(9.8,2.8))\n",
    "#ax = axes.ravel()\n",
    "plt.gray()\n",
    "\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Original')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(binned_binary_im)\n",
    "ax[1].set_title('thresholding')\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(ed_image)\n",
    "ax[2].set_title('ed_image')\n",
    "ax[2].axis('off')\n",
    "ax[3].imshow(ed_binary_im)\n",
    "ax[3].set_title('ed_binary_im')\n",
    "ax[3].axis('off')\n",
    "\n",
    "plt.tight_layout(pad=.2)\n",
    "plt.show()\n",
    "\n",
    "figure, axes = plt.subplots(1, 2, figsize=(10,3))\n",
    "axes[0].hist(image.ravel(), bins=256)\n",
    "#axes[0].axvline(raw_global_thresh, color='r')\n",
    "axes[0].set_title('original image')\n",
    "axes[1].hist(ed_image.ravel(), bins=\"auto\")\n",
    "axes[1].set_title('ed_image')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def show_histogram(fsize, row, ax, i, frame_key, filt):\n",
    "    index_add = arr_length * (row -1)\n",
    "    return_marked = True\n",
    "    if time_array[i] == 0:\n",
    "        final_image = np.zeros((1440,1920))\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    else:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\") --> threshold\", fontsize=10)\n",
    "        threshold_image = threshold_images(fsize, raw_image, i, return_marked, filt)\n",
    "        image = downscale_local_mean(threshold_image, (2,2), cval=1)\n",
    "        final_image = -1*image\n",
    "        if i == 1:\n",
    "            np.set_printoptions(threshold=sys.maxsize)\n",
    "            print(final_image[0:100])\n",
    "    ax.imshow(final_image, cmap = 'gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# For better results, we filter & 'skeletonize' the images before analysis \n",
    "### Do we need to filter out background noise from the images? If so, we can try out different filter sizes to see which works best. The following block previews the images intended for SIA analysis, showing the filtered and skeletonized images\n",
    "'skeletonize' means we find a specific threshold (based on median intensity), so all pixel values above that threshold = 1, and all pixel values below that threshold = 0. This gives structures clean borders for improved correlation analysis, g(r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_key = 0\n",
    "### \"frame_key\" specifies which frame of each tiff file will be analyzed (each frame of my tiff is for a different condition)\n",
    "### e.g. \"key = 0\" dictates that the first frame of each tiff file should be analyzed\n",
    "condition = frame_names[frame_key]\n",
    "print(\"condition: \"+condition)\n",
    "\n",
    "fsize = 1000\n",
    "filt = True\n",
    "### set filter size (pixel area used to estimate and remove average background pixel intensities, recommendation = 600\n",
    "if filt == False:\n",
    "    f = \"none\" \n",
    "else:\n",
    "    f = str(fsize)\n",
    "\n",
    "row = 1\n",
    "### choose which set of tiff files should be analyzed, row1, row2 or row3\n",
    "\n",
    "fig_height = num_rows*2.3\n",
    "time_array.append(0)\n",
    "i = 0\n",
    "fig, axs = plt.subplots(num_rows, 4, figsize=(10,fig_height))\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j % 2 == 0:\n",
    "        show_filtered_images(fsize, row, ax, i, frame_key, filt)\n",
    "    else:\n",
    "        show_threshold_images(fsize, row, ax, i, frame_key, filt)\n",
    "        i = i + 1 \n",
    "plt.show()\n",
    "time_array.remove(0)\n",
    "print(time_array)\n",
    "\n",
    "### option to save this figure (uncomment below)\n",
    "fig.savefig(data_dir2+\"m n threshold vs filtered images for \"+condition+\" (row\"+str(row)+\", f=\"+f+\")\"+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run some SIA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_array.remove(0)\n",
    "print(time_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### \"key\" specifies which frame of each tiff file will be analyzed (each frame of my tiff is for a different condition)\n",
    "### e.g. \"key = 0\" dictates that the first frame of each tiff file should be analyzed, tiff_file.imread(files[i],key=[key]\n",
    "key = 3\n",
    "\n",
    "### cmap dictates the color gradient used in plots; options: 'Reds' 'Blues' 'Greens' 'Greys' 'Purples' ...\n",
    "cmap = matplotlib.cm.get_cmap('Greys')  \n",
    "\n",
    "### If true, filters images to remove excess fluorescense background or other noise\n",
    "t_f = True\n",
    "_bin = True\n",
    "\n",
    "### set filter size (pixel area used to estimate and remove average background pixel intensities, recommendation = 600\n",
    "size = 1000  \n",
    "\n",
    "### Set up empty arrays to save results in\n",
    "im_array = [0.0] * int(arr_length)\n",
    "r1_corr_rad_array = [0.0] * int(arr_length)\n",
    "r2_corr_rad_array = [0.0] * int(arr_length)\n",
    "r3_corr_rad_array = [0.0] * int(arr_length)\n",
    "r4_corr_rad_array = [0.0] * int(arr_length)\n",
    "r5_corr_rad_array = [0.0] * int(arr_length)\n",
    "r6_corr_rad_array = [0.0] * int(arr_length)\n",
    "all_xvalues = [0.0] * int(len(files))\n",
    "\n",
    "### Set basic plot design \n",
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "markerSize = 4\n",
    "\n",
    "### cmap_num is used the when plotting each curve to adjust the color gradient according to the total number of time points\n",
    "### e.g. for i in range(arr_length): the color of each curve is determined by c=cmap(0.9-(i/cmap_num))\n",
    "cmap_num = (arr_length*2) - 2\n",
    "\n",
    "### Set up more empty arrays to save results in\n",
    "mean_corr_rad_array  = [0.0] * int(len(files))\n",
    "std_error_array = [0.0] * int(len(files))\n",
    "\n",
    "for i in range(arr_length):    \n",
    "    frame_num = \"frame %i\" % int(key +1)    ### used in plot title to specify which frame of all tiff files was analyzed\n",
    "    data_file = str(time_array[i]) + \" hrs\" ### used in legend to show time points corresponding to each curve\n",
    "    \n",
    "### \"im_corr\" is the actual SIA function which filters, bins, and fourier transforms tiff images to generate SIA curves\n",
    "    r1_corr_rad_array[i] = SIA(tiff_file.imread(files[i],key=[key]),filter=t_f,fsize=size, bin=_bin, binsize=2)\n",
    "    r2_corr_rad_array[i] = SIA(tiff_file.imread(files[i+arr_length],key=[key]),filter=t_f,fsize=size, bin=_bin, binsize=2)\n",
    "    r3_corr_rad_array[i] = SIA(tiff_file.imread(files[i+(arr_length*2)],key=[key]),filter=t_f,fsize=size, bin=_bin, binsize=2)\n",
    "    r4_corr_rad_array[i] = SIA(tiff_file.imread(files[i+(arr_length*3)],key=[key]),filter=t_f,fsize=size, bin=_bin, binsize=2)\n",
    "    r5_corr_rad_array[i] = SIA(tiff_file.imread(files[i+(arr_length*4)],key=[key]),filter=t_f,fsize=size, bin=_bin, binsize=2)\n",
    "    r6_corr_rad_array[i] = SIA(tiff_file.imread(files[i+(arr_length*5)],key=[key]),filter=t_f,fsize=size, bin=_bin, binsize=2)\n",
    "### corresponding x-values calculated according to the length of a SIA curve array (r1_corr_rad_array[0]) and pixel size\n",
    "    all_xvalues = np.arange(len(r1_corr_rad_array[0]))*pixel_size\n",
    "    \n",
    "### this block calculates the average and std error of the 3 SIA curves (\"r1_corr_rad_array[i]\", \"r2_corr_rad_array[i]\", and \n",
    "### \"r3_corr_rad_array[i]\") generated from the specified frame (key) of 3 tiff files corresponding to time point \"time_array[i]\"\n",
    "    all_ims = np.zeros((6,len(r1_corr_rad_array[i])))\n",
    "    all_ims[0] = r1_corr_rad_array[i]\n",
    "    all_ims[1] = r2_corr_rad_array[i]\n",
    "    all_ims[2] = r3_corr_rad_array[i]\n",
    "    all_ims[3] = r4_corr_rad_array[i]\n",
    "    all_ims[4] = r5_corr_rad_array[i]\n",
    "    all_ims[5] = r6_corr_rad_array[i]\n",
    "    mean_corr_rad_array[i] = all_ims.mean(axis=0)\n",
    "    std_error_array[i] = (all_ims.std(axis=0))/np.sqrt(3)\n",
    "    \n",
    "### plot the average SIA curves for each time point with the std error as error bars \n",
    "    plt.semilogx(all_xvalues, mean_corr_rad_array[i],'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)),label=data_file)\n",
    "    ax.errorbar(all_xvalues, mean_corr_rad_array[i], std_error_array[i], fmt = 'none', elinewidth=0.9, c=cmap(0.9-(i/cmap_num))) \n",
    "    \n",
    "print(\"radial image size: %5.3f x %5.3f um, last x-value= %5.3f\" %(all_xvalues[-1], all_xvalues[-1], all_xvalues[-1]))\n",
    "\n",
    "### legend, labels and title for the plot \n",
    "### **note: the title determined in this code block carries over to all other plots unless otherwise specified \n",
    "plt.xlabel(\"Distance ($\\mu$m)\",fontsize=font_size)\n",
    "plt.ylabel(\"Autocorrelation\",fontsize=font_size) \n",
    "ax.legend(loc=0, markerscale=4.,fontsize=font_size-2)\n",
    "ax.tick_params(direction='in', which='both', labelsize=font_size)\n",
    "\n",
    "### x-axis limit is set slighlty larger than the image size, based on \"all_xvalues[-1]\" \n",
    "plt.xlim(0, all_xvalues[-1] +2) \n",
    "\n",
    "### set title\n",
    "if t_f == False:\n",
    "    fsize = \"None\"\n",
    "else:\n",
    "    fsize = str(size)\n",
    "title = frame_names[key] + ' (' + frame_num + ') filter= ' + fsize\n",
    "plt.title(title, fontsize=font_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save figure \n",
    "fig.savefig(plot_saveto+\"threshold SIA avg, error for \"+title+\".jpg\", dpi=dpi_num)\n",
    "print(plot_saveto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we fit the SIA curves to a single exponential, fit equation: y = e^(-x/L1) + A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "markerSize = 6\n",
    "\n",
    "### Set up dictionary (\"results_dict\") and more empty arrays to save results in\n",
    "results_dict = {}\n",
    "results_dict[\"time array\"] = time_array\n",
    "\n",
    "r1_params = [0.0] * arr_length\n",
    "r2_params = [0.0] * arr_length\n",
    "r3_params = [0.0] * arr_length\n",
    "r4_params = [0.0] * arr_length\n",
    "r5_params = [0.0] * arr_length\n",
    "r6_params = [0.0] * arr_length\n",
    "\n",
    "### FIT PARAMETERS: adjust these to change the range for fitting attempts \n",
    "fit_start = 0\n",
    "fit_lim = -1 \n",
    "retry_num = 10 #retry_num is the number of fitting attempts to try before moving on, not very relevant to single exp fits\n",
    "x_fit_lim = all_xvalues[fit_lim]\n",
    "print(\"first x-value= %5.3f, start fits at %5.3f; fit until xlim = %5.3f\" %(all_xvalues[1], all_xvalues[fit_start], x_fit_lim))\n",
    "print(\"Fits:\")\n",
    "\n",
    "for i in range(arr_length):     \n",
    "    full_filename = files[i]\n",
    "    time = str(time_array[i]) + \" hrs\" #\"time \"+(full_filename.split('\\\\')[-1])[12:-4]\n",
    "\n",
    "### load and plot the 3 original SIA curves corresponding to 3 tiff files associated with each time point \n",
    "    r1_y_array = r1_corr_rad_array[i]\n",
    "    r2_y_array = r2_corr_rad_array[i]\n",
    "    r3_y_array = r3_corr_rad_array[i]\n",
    "    r4_y_array = r4_corr_rad_array[i]\n",
    "    r5_y_array = r5_corr_rad_array[i]\n",
    "    r6_y_array = r6_corr_rad_array[i]\n",
    "    plt.semilogx(all_xvalues,r1_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)),label=time)\n",
    "    #plt.plot(all_xvalues,r2_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    #plt.plot(all_xvalues,r3_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    #plt.plot(all_xvalues,r4_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    #plt.plot(all_xvalues,r5_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    plt.plot(all_xvalues,r6_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    \n",
    "### x_fit_values has the same range as \"all_xvalues\", but includes more values to produce better fits\n",
    "    x_fit_values = np.linspace(all_xvalues[fit_start], all_xvalues[fit_lim], 1000) \n",
    "\n",
    "### the \"curve_fit\" function from scipy does the initial fitting attempt\n",
    "    r1_popt, r1_pcov = curve_fit(single_exponential, all_xvalues[fit_start:fit_lim], r1_y_array[fit_start:fit_lim])\n",
    "    r2_popt, r2_pcov = curve_fit(single_exponential, all_xvalues[fit_start:fit_lim], r2_y_array[fit_start:fit_lim])\n",
    "    r3_popt, r3_pcov = curve_fit(single_exponential, all_xvalues[fit_start:fit_lim], r3_y_array[fit_start:fit_lim])\n",
    "    r4_popt, r4_pcov = curve_fit(single_exponential, all_xvalues[fit_start:fit_lim], r4_y_array[fit_start:fit_lim]) \n",
    "    r5_popt, r5_pcov = curve_fit(single_exponential, all_xvalues[fit_start:fit_lim], r5_y_array[fit_start:fit_lim])\n",
    "    r6_popt, r6_pcov = curve_fit(single_exponential, all_xvalues[fit_start:fit_lim], r6_y_array[fit_start:fit_lim])\n",
    "    print(time+\"-- \"+'r1 fit: A=%5.3f, l1=%5.3f' % tuple(r1_popt))\n",
    "    print(time+\"-- \"+'r2 fit: A=%5.3f, l1=%5.3f' % tuple(r2_popt))\n",
    "    print(time+\"-- \"+'r3 fit: A=%5.3f, l1=%5.3f' % tuple(r3_popt))\n",
    "    print(time+\"-- \"+'r4 fit: A=%5.3f, l1=%5.3f' % tuple(r4_popt))\n",
    "    print(time+\"-- \"+'r5 fit: A=%5.3f, l1=%5.3f' % tuple(r5_popt))\n",
    "    print(time+\"-- \"+'r6 fit: A=%5.3f, l1=%5.3f' % tuple(r6_popt))\n",
    "    r1_A, r1_cl = tuple(r1_popt)\n",
    "    r2_A, r2_cl = tuple(r2_popt)\n",
    "    r3_A, r3_cl = tuple(r3_popt)\n",
    "    r4_A, r4_cl = tuple(r4_popt)\n",
    "    r5_A, r5_cl = tuple(r5_popt)\n",
    "    r6_A, r6_cl = tuple(r6_popt)\n",
    "    \n",
    "### the \"check fits\" function re-runs \"curve_fit\" with a slightly smaller range until the fit parameters meet our criteria\n",
    "### specified in the \"check fits\" function, our fit parameters criteria was more relevant for double exponential fits\n",
    "### (most single exponential fits work first try)\n",
    "    #r1_A, r1_cl = check_fits(r1_A, r1_cl, r1_y_array, retry_num)\n",
    "    #r2_A, r2_cl = check_fits(r2_A, r2_cl, r2_y_array, retry_num)\n",
    "    #r3_A, r3_cl = check_fits(r3_A, r3_cl, r3_y_array, retry_num)\n",
    "    #r4_A, r4_cl = check_fits(r4_A, r4_cl, r4_y_array, retry_num)\n",
    "    #r5_A, r5_cl = check_fits(r5_A, r5_cl, r5_y_array, retry_num)\n",
    "    #r6_A, r6_cl = check_fits(r6_A, r6_cl, r6_y_array, retry_num)\n",
    "### generate and plot curves based on the fits \n",
    "    r1_fit_values = single_exponential(x_fit_values, *r1_popt)\n",
    "    r2_fit_values = single_exponential(x_fit_values, *r2_popt)\n",
    "    r3_fit_values = single_exponential(x_fit_values, *r3_popt)\n",
    "    r4_fit_values = single_exponential(x_fit_values, *r4_popt)\n",
    "    r5_fit_values = single_exponential(x_fit_values, *r5_popt)\n",
    "    r6_fit_values = single_exponential(x_fit_values, *r6_popt)\n",
    "    plt.plot(x_fit_values, r1_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    #plt.plot(x_fit_values, r2_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    #plt.plot(x_fit_values, r3_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    #plt.plot(x_fit_values, r4_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    #plt.plot(x_fit_values, r5_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    plt.plot(x_fit_values, r6_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    \n",
    "### saving all data & results to results dictionary \n",
    "    results_dict[time] = {} #creates sub dictionary (within results_dict) for each frame analyzed \n",
    "    results_dict[time][\"x vals\"] = all_xvalues\n",
    "    results_dict[time][\"r1 y vals\"] = r1_y_array\n",
    "    results_dict[time][\"r2 y vals\"] = r2_y_array\n",
    "    results_dict[time][\"r3 y vals\"] = r3_y_array\n",
    "    results_dict[time][\"r4 y vals\"] = r4_y_array\n",
    "    results_dict[time][\"r5 y vals\"] = r5_y_array\n",
    "    results_dict[time][\"r6 y vals\"] = r6_y_array\n",
    "    \n",
    "    results_dict[time][\"avg y vals\"] = mean_corr_rad_array[i]\n",
    "    results_dict[time][\"avg y error\"] = std_error_array[i]\n",
    "    \n",
    "    results_dict[time][\"x fit vals\"] = x_fit_values\n",
    "    results_dict[time][\"r1 fit vals\"] = r1_fit_values\n",
    "    results_dict[time][\"r2 fit vals\"] = r2_fit_values\n",
    "    results_dict[time][\"r3 fit vals\"] = r3_fit_values\n",
    "    results_dict[time][\"r4 fit vals\"] = r4_fit_values\n",
    "    results_dict[time][\"r5 fit vals\"] = r5_fit_values\n",
    "    results_dict[time][\"r6 fit vals\"] = r6_fit_values\n",
    "    \n",
    "    results_dict[time][\"r1 fit params\"] = [r1_A, r1_cl]\n",
    "    results_dict[time][\"r2 fit params\"] = [r2_A, r2_cl]\n",
    "    results_dict[time][\"r3 fit params\"] = [r3_A, r3_cl]\n",
    "    results_dict[time][\"r4 fit params\"] = [r4_A, r4_cl]\n",
    "    results_dict[time][\"r5 fit params\"] = [r5_A, r5_cl]\n",
    "    results_dict[time][\"r6 fit params\"] = [r6_A, r6_cl]\n",
    "    \n",
    "    r1_params[i] = (r1_A, r1_cl)\n",
    "    r2_params[i] = (r2_A, r2_cl) \n",
    "    r3_params[i] = (r3_A, r3_cl)\n",
    "    r4_params[i] = (r4_A, r4_cl)\n",
    "    r5_params[i] = (r5_A, r5_cl) \n",
    "    r6_params[i] = (r6_A, r6_cl)\n",
    "    #print(\"br_yarray[0] = %5.3f, br_yarray[1] = %5.3f, br_fit_values[0] = %5.3f, br_fit_values[1] = %5.3f\" %(br_y_array[0], \n",
    "      #                                                                                                   br_y_array[1],\n",
    "       #                                                                                                  br_fit_values[0], \n",
    "                #                                                                                         br_fit_values[1]))\n",
    "    \n",
    "plt.xlabel(\"Distance ($\\mu$m)\",fontsize=font_size)\n",
    "plt.ylabel(\"Autocorrelation\",fontsize=font_size)\n",
    "ax.legend(loc=0, markerscale=2.,fontsize=font_size-3)\n",
    "#plt.ylim(0.1, 1.1)\n",
    "#plt.ylim(0.006, 1.01)\n",
    "#plt.xlim(0, 40)\n",
    "#plt.xlim(0, all_xvalues[-10]+5)\n",
    "plt.xlim(0, all_xvalues[fit_lim]+5) ## (0.091 um/px) * (1440 px) = 131.04 um --> =size of image x-axis in microns\n",
    "str_equation = True\n",
    "equation = single_exponential(x_fit_values, *r1_popt)\n",
    "ax.text(1.2,0.95, \"fit equation: \" + equation, fontsize=font_size-2)\n",
    "str_equation = False\n",
    "\n",
    "title_plus = title + \"; new fits range (%5.3f um, %5.3f um)\" %(all_xvalues[fit_start], x_fit_lim)\n",
    "plt.title(title_plus, fontsize=font_size -4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save figure\n",
    "fig.savefig(plot_saveto+\"threshold SIA fits for \"+title_plus+\".jpg\", dpi=dpi_num)\n",
    "###save dictionary results \n",
    "file_to_write = open(plot_saveto+ \"threshold SIA results for \"+title+\".p\", \"wb\")\n",
    "#file_to_write = open(plot_saveto+ \"SIA results for \"+title_plus+\".p\", \"wb\")\n",
    "pickle.dump(results_dict, file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine and plot the results of fitting SIA curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "array_len = len(r1_params)\n",
    "### set up empty arrays\n",
    "all_cl = np.zeros((6,array_len))\n",
    "avg_cl = np.empty(array_len)\n",
    "stderror_cl = np.empty(array_len)\n",
    "\n",
    "### find average values and std. error for correlation lengths (lc) based on the fits \n",
    "for i in range(array_len):\n",
    "    all_cl[0,i] = r1_params[i][1]\n",
    "    all_cl[1,i] = r2_params[i][1]\n",
    "    all_cl[2,i] = r3_params[i][1]\n",
    "    all_cl[3,i] = r4_params[i][1]\n",
    "    all_cl[4,i] = r5_params[i][1]\n",
    "    all_cl[5,i] = r6_params[i][1]\n",
    "avg_cl = all_cl.mean(axis=0)\n",
    "stderror_cl = (all_cl.std(axis=0))/np.sqrt(6)   #\n",
    "#print(all_cl)\n",
    "#print(all_cl.mean(axis=0))\n",
    "#print(stderror_cl)\n",
    "\n",
    "### plot average values and std. error for correlation lengths\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "markerSize = 8\n",
    "\n",
    "for i in range(int(len(time_array))):\n",
    "    plt.plot(time_array[i], avg_cl[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)))\n",
    "    ax.errorbar(time_array[i], avg_cl[i], yerr = stderror_cl[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "plt.xlabel(\"Time (hrs after adding KaiC)\",fontsize=font_size)\n",
    "plt.ylabel(\"correlation length ($\\mu$m)\",fontsize=font_size)\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "plt.ylim(1,6)\n",
    "title_plus = title + \"; fits range (%5.3f um, %5.3f um)\" %(all_xvalues[fit_start], x_fit_lim)\n",
    "plt.title(title_plus, fontsize=font_size -4)\n",
    "plt.show()\n",
    "\n",
    "### save plot\n",
    "fig.savefig(plot_saveto+\"n threshold correlation lengths for \"+title_plus+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_len = len(r1_params)\n",
    "### set up empty arrays\n",
    "all_A = np.zeros((6,array_len))\n",
    "avg_A = np.empty(array_len)\n",
    "stderror_A = np.empty(array_len)\n",
    "\n",
    "### find average values and std. error for correlation lengths (lc) based on the fits \n",
    "for i in range(array_len):\n",
    "    all_A[0,i] = r1_params[i][0]\n",
    "    all_A[1,i] = r2_params[i][0]\n",
    "    all_A[2,i] = r3_params[i][0]\n",
    "    all_A[3,i] = r4_params[i][0]\n",
    "    all_A[4,i] = r5_params[i][0]\n",
    "    all_A[5,i] = r6_params[i][0]\n",
    "avg_A = all_A.mean(axis=0)\n",
    "stderror_A = (all_A.std(axis=0))/np.sqrt(6)   #\n",
    "#print(all_cl)\n",
    "#print(all_cl.mean(axis=0))\n",
    "#print(stderror_cl)\n",
    "\n",
    "### plot average values and std. error for correlation lengths\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "markerSize = 8\n",
    "\n",
    "for i in range(int(len(time_array))):\n",
    "    plt.plot(time_array[i], avg_A[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)))\n",
    "    ax.errorbar(time_array[i], avg_A[i], yerr = stderror_A[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "plt.xlabel(\"time (hrs after adding KaiC)\",fontsize=font_size)\n",
    "plt.ylabel(\"offset parameter (... + A) value (unitless)\",fontsize=font_size)\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "plt.ylim(-0.01,0.02)\n",
    "title_plus = title + \"; fits range (%5.3f um, %5.3f um)\" %(all_xvalues[fit_start], x_fit_lim)\n",
    "plt.title(title_plus, fontsize=font_size -4)\n",
    "plt.show()\n",
    "\n",
    "### save plot\n",
    "fig.savefig(plot_saveto+\"n threshold _offset parameter values for \"+title_plus+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all results to 3 seperate CSV files --> use for plotting in origin later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### first CSV file: save fit parameters (coefficient 'A' and correlation length 'L1'), and avg autocorrelation plateau values\n",
    "csv_data_file = \"results- threshold SIA fit parameters.csv\"\n",
    "data_file_exists = os.path.isfile(plot_saveto+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(csv_data_file + \" already exists.\")\n",
    "    with open(plot_saveto+csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title_plus])\n",
    "        writer.writerow(['time (hrs)','','r1 A','r2 A','r3 A','r4 A','r5 A','r6 A','','avg A', 'A std E',\n",
    "                         '','','r1 cL','r2 cL','r3 cL','r4 cL','r5 cL','r6 cL','', 'avg cL', 'cL std E'])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i],'', r1_params[i][0], r2_params[i][0], r3_params[i][0], r4_params[i][0], \n",
    "                             r5_params[i][0], r6_params[i][0],'', avg_A[i], stderror_A[i],'','',\n",
    "                             r1_params[i][1], r2_params[i][1], r3_params[i][1], r4_params[i][1], r5_params[i][1], \n",
    "                             r6_params[i][1],'', avg_cl[i], stderror_cl[i]])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"Results appended to \"+ csv_data_file)\n",
    "else:\n",
    "    print(csv_data_file + \" does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(plot_saveto+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(plot_saveto+csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title_plus])\n",
    "        writer.writerow(['time (hrs)','','r1 A','r2 A','r3 A','r4 A','r5 A','r6 A','','avg A', 'A std E',\n",
    "                         '','','r1 cL','r2 cL','r3 cL','r4 cL','r5 cL','r6 cL','', 'avg cL', 'cL std E'])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i],'', r1_params[i][0], r2_params[i][0], r3_params[i][0], r4_params[i][0], \n",
    "                             r5_params[i][0], r6_params[i][0],'', avg_A[i], stderror_A[i],'','',\n",
    "                             r1_params[i][1], r2_params[i][1], r3_params[i][1], r4_params[i][1], r5_params[i][1], \n",
    "                             r6_params[i][1],'', avg_cl[i], stderror_cl[i]])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to \"+ csv_data_file)\n",
    "\n",
    "### BOX PLOTS CSV file: save fit parameters (coefficient 'A' and correlation length 'L1'), and avg autocorrelation plateau values\n",
    "csv_data_file = \"Box Plot results- threshold SIA fit parameters.csv\"\n",
    "data_file_exists = os.path.isfile(plot_saveto+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(csv_data_file + \" already exists.\")\n",
    "    with open(plot_saveto+csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title_plus])\n",
    "        writer.writerow(['','A values','','','','','','','','','','','','correlation length'])\n",
    "        writer.writerow(['time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',])\n",
    "        \n",
    "        writer.writerow(['row1', r1_params[1][0], r1_params[2][0], r1_params[3][0], r1_params[4][0], r1_params[5][0],\n",
    "                         r1_params[6][0], r1_params[7][0], r1_params[8][0], r1_params[i][0],'','',\n",
    "                         'row1', r1_params[1][1], r1_params[2][1], r1_params[3][1], r1_params[4][1], r1_params[5][1],\n",
    "                         r1_params[6][1], r1_params[7][1], r1_params[8][1], r1_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row2', r2_params[1][0], r2_params[2][0], r2_params[3][0], r2_params[4][0], r2_params[5][0],\n",
    "                         r2_params[6][0], r2_params[7][0], r2_params[8][0], r2_params[i][0],'','',\n",
    "                         'row2', r2_params[1][1], r2_params[2][1], r2_params[3][1], r2_params[4][1], r2_params[5][1],\n",
    "                         r2_params[6][1], r2_params[7][1], r2_params[8][1], r2_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row3', r3_params[1][0], r3_params[2][0], r3_params[3][0], r3_params[4][0], r3_params[5][0],\n",
    "                         r3_params[6][0], r3_params[7][0], r3_params[8][0], r3_params[i][0],'','',\n",
    "                         'row3', r2_params[1][1], r2_params[2][1], r2_params[3][1], r2_params[4][1], r2_params[5][1],\n",
    "                         r3_params[6][1], r3_params[7][1], r3_params[8][1], r3_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row4', r4_params[1][0], r4_params[2][0], r4_params[3][0], r4_params[4][0], r4_params[5][0],\n",
    "                         r4_params[6][0], r4_params[7][0], r4_params[8][0], r4_params[i][0],'','',\n",
    "                         'row4', r4_params[1][1], r4_params[2][1], r4_params[3][1], r4_params[4][1], r4_params[5][1],\n",
    "                         r4_params[6][1], r4_params[7][1], r4_params[8][1], r4_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row5', r5_params[1][0], r5_params[2][0], r5_params[3][0], r5_params[4][0], r5_params[5][0],\n",
    "                         r5_params[6][0], r5_params[7][0], r5_params[8][0], r5_params[i][0],'','',\n",
    "                         'row5', r5_params[1][1], r5_params[2][1], r5_params[3][1], r5_params[4][1], r5_params[5][1],\n",
    "                         r5_params[6][1], r5_params[7][1], r5_params[8][1], r5_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row6', r6_params[1][0], r6_params[2][0], r6_params[3][0], r6_params[4][0], r6_params[5][0],\n",
    "                         r6_params[6][0], r6_params[7][0], r6_params[8][0], r6_params[i][0],'','',\n",
    "                         'row6', r6_params[1][1], r6_params[2][1], r6_params[3][1], r6_params[4][1], r6_params[5][1],\n",
    "                         r6_params[6][1], r6_params[7][1], r6_params[8][1], r6_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"Results appended to \"+ csv_data_file)\n",
    "else:\n",
    "    print(csv_data_file + \" does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(plot_saveto+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(plot_saveto+csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title_plus])\n",
    "        writer.writerow(['','A values','','','','','','','','','','','','correlation length'])\n",
    "        writer.writerow(['time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',])\n",
    "        \n",
    "        writer.writerow(['row1', r1_params[1][0], r1_params[2][0], r1_params[3][0], r1_params[4][0], r1_params[5][0],\n",
    "                         r1_params[6][0], r1_params[7][0], r1_params[8][0], r1_params[i][0],'','',\n",
    "                         'row1', r1_params[1][1], r1_params[2][1], r1_params[3][1], r1_params[4][1], r1_params[5][1],\n",
    "                         r1_params[6][1], r1_params[7][1], r1_params[8][1], r1_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row2', r2_params[1][0], r2_params[2][0], r2_params[3][0], r2_params[4][0], r2_params[5][0],\n",
    "                         r2_params[6][0], r2_params[7][0], r2_params[8][0], r2_params[i][0],'','',\n",
    "                         'row2', r2_params[1][1], r2_params[2][1], r2_params[3][1], r2_params[4][1], r2_params[5][1],\n",
    "                         r2_params[6][1], r2_params[7][1], r2_params[8][1], r2_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row3', r3_params[1][0], r3_params[2][0], r3_params[3][0], r3_params[4][0], r3_params[5][0],\n",
    "                         r3_params[6][0], r3_params[7][0], r3_params[8][0], r3_params[i][0],'','',\n",
    "                         'row3', r2_params[1][1], r2_params[2][1], r2_params[3][1], r2_params[4][1], r2_params[5][1],\n",
    "                         r3_params[6][1], r3_params[7][1], r3_params[8][1], r3_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row4', r4_params[1][0], r4_params[2][0], r4_params[3][0], r4_params[4][0], r4_params[5][0],\n",
    "                         r4_params[6][0], r4_params[7][0], r4_params[8][0], r4_params[i][0],'','',\n",
    "                         'row4', r4_params[1][1], r4_params[2][1], r4_params[3][1], r4_params[4][1], r4_params[5][1],\n",
    "                         r4_params[6][1], r4_params[7][1], r4_params[8][1], r4_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row5', r5_params[1][0], r5_params[2][0], r5_params[3][0], r5_params[4][0], r5_params[5][0],\n",
    "                         r5_params[6][0], r5_params[7][0], r5_params[8][0], r5_params[i][0],'','',\n",
    "                         'row5', r5_params[1][1], r5_params[2][1], r5_params[3][1], r5_params[4][1], r5_params[5][1],\n",
    "                         r5_params[6][1], r5_params[7][1], r5_params[8][1], r5_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow(['row6', r6_params[1][0], r6_params[2][0], r6_params[3][0], r6_params[4][0], r6_params[5][0],\n",
    "                         r6_params[6][0], r6_params[7][0], r6_params[8][0], r6_params[i][0],'','',\n",
    "                         'row6', r6_params[1][1], r6_params[2][1], r6_params[3][1], r6_params[4][1], r6_params[5][1],\n",
    "                         r6_params[6][1], r6_params[7][1], r6_params[8][1], r6_params[i][1],'','',])\n",
    "        \n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to \"+ csv_data_file)\n",
    "    \n",
    "### second CSV file: save raw data (x and y values) of each original SIA curve (3 curves for each time point)\n",
    "csv_data_file = \"results- threshold SIA raw data\"+title+\".csv\"   \n",
    "header = []\n",
    "np.savetxt(plot_saveto+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "with open(plot_saveto+csv_data_file,'a', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for j in range(len(time_array)):\n",
    "        time = str(time_array[j]) + ' hrs'\n",
    "        writer.writerow([time])\n",
    "        writer.writerow(['x vals','','r1 y vals','r2 y vals','r3 y vals','r4 y vals','r5 y vals','r6 y vals',\n",
    "                         '','avg y vals','avg y error'])\n",
    "        for i in range(len(results_dict[time]['x vals'])):\n",
    "            writer.writerow([results_dict[time]['x vals'][i],'', results_dict[time]['r1 y vals'][i],\n",
    "                             results_dict[time]['r2 y vals'][i], results_dict[time]['r3 y vals'][i], \n",
    "                             results_dict[time]['r4 y vals'][i], results_dict[time]['r5 y vals'][i],\n",
    "                             results_dict[time]['r6 y vals'][i],\n",
    "                             '', results_dict[time]['avg y vals'][i], results_dict[time]['avg y error'][i]])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "f.close()\n",
    "\n",
    "### third CSV file: save the raw data (x and y values) of each fit to each original SIA curve (3 curves for each time point)\n",
    "csv_data_file = \"results- threshold SIA fits data\"+title+\".csv\"\n",
    "header = []\n",
    "np.savetxt(plot_saveto+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "with open(plot_saveto+csv_data_file,'a', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for j in range(len(time_array)):\n",
    "        time = str(time_array[j]) + ' hrs'\n",
    "        writer.writerow([time])\n",
    "        writer.writerow(['x fit vals','','r1 fit vals','r2 fit vals','r3 fit vals','r4 fit vals','r5 fit vals','r6 fit vals'])\n",
    "        for i in range(len(results_dict[time]['x fit vals'])):\n",
    "            writer.writerow([results_dict[time]['x fit vals'][i], '', \n",
    "                             results_dict[time]['r1 fit vals'][i], results_dict[time]['r2 fit vals'][i], \n",
    "                             results_dict[time]['r3 fit vals'][i], results_dict[time]['r4 fit vals'][i],\n",
    "                             results_dict[time]['r5 fit vals'][i], results_dict[time]['r6 fit vals'][i]])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First round complete! \n",
    "## Now we can scroll back to the top and change the 'key' variable to run through the next frame (i.e. condition) of all tiff files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
