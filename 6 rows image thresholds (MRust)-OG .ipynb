{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a533f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from numpy.fft import fft2, ifft2, fftshift\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import gaussian_filter1d as gf1d\n",
    "from scipy.ndimage import gaussian_filter as gf\n",
    "from scipy.ndimage import uniform_filter as uf\n",
    "from skimage.transform import downscale_local_mean #For binning\n",
    "from skimage.transform import downscale_local_mean #For binning\n",
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "from statistics import median\n",
    "from statistics import mode\n",
    "\n",
    "import xarray as xr #package for labeling and adding metadata to multi-dimensional arrays\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"../kai_colloids/PyDDM\") #must point to the PyDDM folder\n",
    "#import ddm_analysis_and_fitting as ddm   \n",
    "\n",
    "import tiff_file \n",
    "import io \n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import glob #glob is helpful for searching for filenames or directories\n",
    "import pickle #for saving data\n",
    "### usually this block prints out \"nd2reader module not found. Reading of .nd2 files disabled.\" on the first run\n",
    "### this is fine (unless you need to read .nd2 files), just re-run this block to make the error go away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0792c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "def rows_n_cols(im):\n",
    "    imarray = np.array(im)\n",
    "    rows, cols = np.shape(imarray)\n",
    "    return rows, cols\n",
    "\n",
    "def show_raw_images(row, ax, i, frame_key):\n",
    "    plt.gray()\n",
    "    index_add = arr_length * (row -1)\n",
    "    if time_array[i] == 0:\n",
    "        test_image = np.zeros((1440,1920))\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    else:\n",
    "        test_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\")\", fontsize=10)\n",
    "    ax.imshow(test_image) #cmap = 'gray'\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)\n",
    "    \n",
    "def show_threshold_images(row, ax, i, frame_key, block_size, offset_val):\n",
    "    index_add = arr_length * (row -1)\n",
    "    if time_array[i] == 0:\n",
    "        threshold_image = np.zeros((1440,1920))\n",
    "        ax.set_title('[no image]', fontsize=10)\n",
    "    else:\n",
    "        raw_image = tiff_file.imread(files[i+index_add],key=[frame_key])\n",
    "        ax.set_title(\"~\" + str(time_array[i]) + \" hrs (row\"+str(row)+\") --> threshold\", fontsize=10)\n",
    "        threshold_image = threshold_images(raw_image, block_size, offset_val)\n",
    "    ax.imshow(threshold_image, cmap = 'gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=.2)\n",
    "    \n",
    "def floodfill_flat(array, x, y, rows, cols):\n",
    "    stack = []\n",
    "    if array[x,y] != 1:\n",
    "        return 0\n",
    "    stack.append((x,y))\n",
    "    currclustersize = 1\n",
    "    array[x,y] = 2\n",
    "    while stack:\n",
    "        xcurr, ycurr = stack.pop()\n",
    "        if xcurr > 0:\n",
    "            if array[xcurr-1,ycurr] == 1:\n",
    "                array[xcurr-1,ycurr] = 2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr-1,ycurr))\n",
    "        if xcurr < rows-1:\n",
    "            if array[xcurr+1,ycurr] == 1:\n",
    "                array[xcurr+1,ycurr] = 2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr+1,ycurr))\n",
    "        if ycurr > 0:\n",
    "            if array[xcurr,ycurr-1] == 1:\n",
    "                array[xcurr,ycurr-1] = 2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr,ycurr-1))\n",
    "        if ycurr < cols-1:\n",
    "            if array[xcurr,ycurr+1] == 1:\n",
    "                array[xcurr,ycurr+1] = 2\n",
    "                currclustersize += 1\n",
    "                stack.append((xcurr,ycurr+1))\n",
    "    return currclustersize\n",
    "\n",
    "def markcluster_flat(array, x, y, rows, cols, clustersize):\n",
    "    stack = []\n",
    "    if array[x,y] != 2:\n",
    "        return\n",
    "    stack.append((x,y))\n",
    "    array[x,y] = clustersize\n",
    "    while stack:\n",
    "        xcurr, ycurr = stack.pop()\n",
    "        if xcurr > 0:\n",
    "            if array[xcurr-1,ycurr] == 2:\n",
    "                array[xcurr-1,ycurr] = clustersize\n",
    "                stack.append((xcurr-1,ycurr))\n",
    "        if xcurr < rows-1:\n",
    "            if array[xcurr+1,ycurr] == 2:\n",
    "                array[xcurr+1,ycurr] = clustersize\n",
    "                stack.append((xcurr+1,ycurr))\n",
    "        if ycurr > 0:\n",
    "            if array[xcurr,ycurr-1] == 2:\n",
    "                array[xcurr,ycurr-1] = clustersize\n",
    "                stack.append((xcurr,ycurr-1))\n",
    "        if ycurr < cols-1:\n",
    "            if array[xcurr,ycurr+1] == 2:\n",
    "                array[xcurr,ycurr+1] = clustersize\n",
    "                stack.append((xcurr,ycurr+1))\n",
    "\n",
    "def threshold_images(image, block_size, offset_val):\n",
    "    ed_image = (image**0.2)*500\n",
    "    ed_thresh = threshold_local(ed_image, block_size, offset= offset_val)\n",
    "    #ed_binary_im = (ed_image > ed_thresh)\n",
    "    #print(ed_binary_im[:2])\n",
    "    ed_binary_im = 1*(ed_image > ed_thresh)\n",
    "    #print(ed_binary_im[:2])\n",
    "    #print(ed_binary_im.shape)\n",
    "    return ed_binary_im\n",
    "\n",
    "def cluster_analysis(size_lim, i, row_num, arraythresh, rows, cols):\n",
    "    clusters = []\n",
    "    for x in range(rows):\n",
    "        #if x % 25 == 0:  #progress update\n",
    "        #    print(100*x/rows)\n",
    "        for y in range(cols):\n",
    "            if arraythresh[x,y] == 1:\n",
    "                #fill in this cluster\n",
    "                clustersize = floodfill_flat(arraythresh, x, y, rows, cols)\n",
    "                clusters.append(clustersize)\n",
    "                #print(\"clustersize= %5.3f for (x,y)= (%5.3f, %5.3f)\"  %(clustersize,x,y))\n",
    "                #print(arraythresh[:2])\n",
    "                #mark finished cluster with size\n",
    "                markcluster_flat(arraythresh,x,y,rows,cols,clustersize)\n",
    "                \n",
    "    for q in range(clusters.count(1)):\n",
    "        clusters.remove(1)\n",
    "    \n",
    "    print(clusters)\n",
    "    clusters.sort()\n",
    "    print(clusters)\n",
    "    if clusters[0] < size_lim:\n",
    "        keep_going = True\n",
    "        while keep_going:\n",
    "            if clusters[0] < size_lim:\n",
    "                m = clusters[0]\n",
    "                clusters.remove(m)\n",
    "            else:\n",
    "                keep_going = False\n",
    "                                                #max_size = (max(clusters)) * (pixel_size**2)  \n",
    "    total_num = len(clusters)\n",
    "    mean_size = (sum(clusters) / len(clusters)) * (pixel_size**2)  #pixel size squared b/c max size is an area\n",
    "    median_size = (median(clusters)) * (pixel_size**2)\n",
    "    mode_size = (mode(clusters)) * (pixel_size**2)\n",
    "    \n",
    "    if (i == 0) and (row_num == 1):\n",
    "        print(clusters)\n",
    "    \n",
    "    print(\"for row\"+str(row_num)+\", t= \"+str(time_array[i]) + \" hrs:\")\n",
    "    print('   clusters counted: %5i, mean size: %5.3f, median: %5.3f, mode: %5.3f' %(total_num, mean_size, \n",
    "                                                                                       median_size, mode_size))\n",
    "    \n",
    "    return total_num, mean_size, median_size, mode_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6c74bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 3, 53, 634, 567, 567, 3456, 464, 65, 6, 6, 5, 34, 3456, 67, 6, 4, 3, 554, 65, 7, 544, 5, 7, 7, 2, 34, 42, 3, 3, 23, 17, 24, 5, 10, 15, 20]\n",
      "[1, 2, 2, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 10, 15, 17, 20, 23, 24, 34, 34, 42, 53, 65, 65, 67, 464, 544, 554, 567, 567, 634, 3456, 3456]\n"
     ]
    }
   ],
   "source": [
    "clust = [1,2,3,4,5,6,7,3,53,634,567,567,3456,464,65,6,6,5,34,3456,67,6,4,3,554,65,7,544,5,7,7,2,34,42,3,3,23,17,24,5,10,15,20]\n",
    "sorted(clust)\n",
    "print(clust)\n",
    "print(sorted(clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424d2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 34, 42, 53, 65, 65, 67, 464, 544, 554, 567, 567, 634, 3456, 3456]\n"
     ]
    }
   ],
   "source": [
    "clust.sort()\n",
    "if clust[0] < 30:\n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "        if clust[0] < 30:\n",
    "            m = clust[0]\n",
    "            clust.remove(m)\n",
    "        else:\n",
    "            keep_going = False\n",
    "            print(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0ce838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 54 files\n",
      " 0 \t row1_t1.tif\n",
      " 1 \t row1_t2.tif\n",
      " 2 \t row1_t3.tif\n",
      " 3 \t row1_t4.tif\n",
      " 4 \t row1_t5.tif\n",
      " 5 \t row1_t6.tif\n",
      " 6 \t row1_t7.tif\n",
      " 7 \t row1_t8.tif\n",
      " 8 \t row1_t9.tif\n",
      " 9 \t row2_t1.tif\n",
      " 10 \t row2_t2.tif\n",
      " 11 \t row2_t3.tif\n",
      " 12 \t row2_t4.tif\n",
      " 13 \t row2_t5.tif\n",
      " 14 \t row2_t6.tif\n",
      " 15 \t row2_t7.tif\n",
      " 16 \t row2_t8.tif\n",
      " 17 \t row2_t9.tif\n",
      " 18 \t row3_t1.tif\n",
      " 19 \t row3_t2.tif\n",
      " 20 \t row3_t3.tif\n",
      " 21 \t row3_t4.tif\n",
      " 22 \t row3_t5.tif\n",
      " 23 \t row3_t6.tif\n",
      " 24 \t row3_t7.tif\n",
      " 25 \t row3_t8.tif\n",
      " 26 \t row3_t9.tif\n",
      " 27 \t row4_t1.tif\n",
      " 28 \t row4_t2.tif\n",
      " 29 \t row4_t3.tif\n",
      " 30 \t row4_t4.tif\n",
      " 31 \t row4_t5.tif\n",
      " 32 \t row4_t6.tif\n",
      " 33 \t row4_t7.tif\n",
      " 34 \t row4_t8.tif\n",
      " 35 \t row4_t9.tif\n",
      " 36 \t row5_t1.tif\n",
      " 37 \t row5_t2.tif\n",
      " 38 \t row5_t3.tif\n",
      " 39 \t row5_t4.tif\n",
      " 40 \t row5_t5.tif\n",
      " 41 \t row5_t6.tif\n",
      " 42 \t row5_t7.tif\n",
      " 43 \t row5_t8.tif\n",
      " 44 \t row5_t9.tif\n",
      " 45 \t row6_t1.tif\n",
      " 46 \t row6_t2.tif\n",
      " 47 \t row6_t3.tif\n",
      " 48 \t row6_t4.tif\n",
      " 49 \t row6_t5.tif\n",
      " 50 \t row6_t6.tif\n",
      " 51 \t row6_t7.tif\n",
      " 52 \t row6_t8.tif\n",
      " 53 \t row6_t9.tif\n"
     ]
    }
   ],
   "source": [
    "directory = \"Z\"\n",
    "exp = \"10-15-22_s1_theBigOne2\"\n",
    "### \"data_dir\" is the pathway to the folder holding the tiff files to be analyzed \n",
    "data_dir = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\all tiff files\\\\\"\n",
    "### \"plot_saveto\" is the pathway to the folder where plots and results will be saved\n",
    "saveto_dir = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\threshold results\\\\\"\n",
    "\n",
    "files = glob.glob(data_dir+\"*_t*\") ### this should generate an ordered list of files in \"data_dir\" which have \"_t\" in their name\n",
    "print(\"found %i files\" % len(files))\n",
    "for i,f in enumerate(files): print (' %i \\t %s' % (i, f.split('\\\\')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c783b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gleech\\Documents\\GitHub\\kai codes\\cluster-analysis\\tiff_file.py:724: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = numpy.vstack((p.asarray() if p else nopage)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiff file dimensions: (5, 1440, 1920)\n",
      "total number of time points: 9\n",
      "(for image previews) number of rows = 5\n"
     ]
    }
   ],
   "source": [
    "frame_names = [\"1-3 kA-WT\", \"WT (no kA)\", \"EA KaiC\", \"AE KaiC\"]\n",
    "### array containing the name for each frame in a tiff file to be run \n",
    "### e.g. frame 1 is an image of the \"50% bKaiB\" condition, frame 2 is an image of the \"35% bKaiB\" condition, etc.\n",
    "\n",
    "time_array = [1.1, 4.0, 7.4, 10.3, 14.7, 17.7, 20.3, 23.8, 27.0]\n",
    "total_rows = 6\n",
    "#s1 [1.1, 4.0, 7.4, 10.3, 14.7, 17.7, 20.3, 23.8, 27.0]\n",
    "#s2 [1.2, 4.4, 7.8, 10.8, 15.2, 18.3, 20.8, 24.3, 28.0]\n",
    "### array containing the time points corresponding to consecutive tiff files\n",
    "### e.g. tiff files \"bottom_row_t1\", \"middle_row_t1\", and \"top_row_t1\" all correspond to t = 0.5 hrs, time_array[0]\n",
    "\n",
    "pixel_size = 0.091 \n",
    "### pixel size (microns per pixel) of frames/ images in the tiff files --- 40x olympus objective => 0.091 um/px\n",
    "### IF 2x2 BINNING: multiply the original pixel size by 2^2 = 4, e.g. 4*(0.091 um/px) = 0.364 um/px\n",
    "\n",
    "eg_im= tiff_file.imread(files[0])\n",
    "print(\"tiff file dimensions: \"+ str(eg_im.shape))\n",
    "\n",
    "arr_length = int((len(files))/6)\n",
    "print(\"total number of time points: \"+ str(arr_length))\n",
    "if arr_length % 2 == 0:\n",
    "    num_rows = int(arr_length/2)\n",
    "else:\n",
    "    num_rows = int((arr_length+1)/2)\n",
    "print(\"(for image previews) number of rows = \" + str(num_rows))\n",
    "\n",
    "font_size = 16  ### font size\n",
    "dpi_num = 600  ### image quality level (recommendation: 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e540a19",
   "metadata": {},
   "source": [
    "# For better results, we filter & 'skeletonize' the images before analysis \n",
    "### Do we need to filter out background noise from the images? If so, we can try out different filter sizes to see which works best. The following block previews the images intended for SIA analysis, showing the filtered and skeletonized images\n",
    "'skeletonize' means we find a specific threshold (based on median intensity), so all pixel values above that threshold = 1, and all pixel values below that threshold = 0. This gives structures clean borders for improved correlation analysis, g(r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef7ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_key = 0\n",
    "### \"frame_key\" specifies which frame of each tiff file will be analyzed (each frame of my tiff is for a different condition)\n",
    "### e.g. \"key = 0\" dictates that the first frame of each tiff file should be analyzed\n",
    "condition = frame_names[frame_key]\n",
    "print(\"condition: \"+condition)\n",
    "\n",
    "block_size = 1051\n",
    "offset_val = -20\n",
    "\n",
    "row = 1\n",
    "### choose which set of tiff files should be analyzed, row1, row2 or row3\n",
    "\n",
    "fig_height = num_rows*2.3\n",
    "time_array.append(0)\n",
    "i = 0\n",
    "fig, axs = plt.subplots(num_rows, 4, figsize=(10,fig_height))\n",
    "for j, ax in enumerate(axs.flatten()):\n",
    "    if j % 2 == 0:\n",
    "        show_raw_images(row, ax, i, frame_key)\n",
    "    else:\n",
    "        show_threshold_images(row, ax, i, frame_key, block_size, offset_val)\n",
    "        i = i + 1 \n",
    "plt.show()\n",
    "time_array.remove(0)\n",
    "print(time_array)\n",
    "details = \" (row\"+str(row)+\", bsize= \"+str(block_size)+\", offset= \"+str(offset_val)+\")\"\n",
    "\n",
    "### option to save this figure (uncomment below)\n",
    "#fig.savefig(saveto_dir+\"threshold images for \"+condition+details+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744c51d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row1_thresh = [0.0] * int(arr_length)\n",
    "row2_thresh = [0.0] * int(arr_length)\n",
    "row3_thresh = [0.0] * int(arr_length)\n",
    "row4_thresh = [0.0] * int(arr_length)\n",
    "row5_thresh = [0.0] * int(arr_length)\n",
    "row6_thresh = [0.0] * int(arr_length)\n",
    "\n",
    "row1_results = [0.0] * int(arr_length)\n",
    "row2_results = [0.0] * int(arr_length)\n",
    "row3_results = [0.0] * int(arr_length)\n",
    "row4_results = [0.0] * int(arr_length)\n",
    "row5_results = [0.0] * int(arr_length)\n",
    "row6_results = [0.0] * int(arr_length)\n",
    "print(\"condition: \"+condition)\n",
    "rows, cols, = rows_n_cols(threshold_images(tiff_file.imread(files[0],key=[frame_key]), block_size, offset_val))\n",
    "#print(rows, cols)\n",
    "\n",
    "beads_per_cluster = 5\n",
    "size_lim = int( (beads_per_cluster*0.785) / (pixel_size**2) ) + 2 #add 2 to round up just for good measure (i.e. 473+2=475)     \n",
    "### 2D bead area = pi*(0.5 um)^2 = 0.785 um^2\n",
    "# e.g (5 beads)(0.785 um^2) = 3.93 um^2 ---> (3.93 um^2)/((0.091 um/px)^2) = ~475 pixels^2\n",
    "print(\"define clusters as at least: \"+str(beads_per_cluster)+\" beads --> size_lim = \"+str(size_lim)+\" pixels^2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2238d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row1_thresh[0] = threshold_images(tiff_file.imread(files[i],key=[frame_key]), block_size, offset_val)\n",
    "r1_total_num, r1_mean_size, r1_median_size, r1_mode_size = cluster_analysis(size_lim, i, 1, row1_thresh[0], rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c66e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#frame_key = 0\n",
    "cmap = matplotlib.cm.get_cmap('Reds')\n",
    "\n",
    "block_size = 1051\n",
    "offset_val = -20\n",
    "\n",
    "for i in range(arr_length):\n",
    "    row1_thresh[i] = threshold_images(tiff_file.imread(files[i],key=[frame_key]), block_size, offset_val)\n",
    "    row2_thresh[i] = threshold_images(tiff_file.imread(files[i+arr_length], key=[frame_key]), block_size, offset_val)\n",
    "    row3_thresh[i] = threshold_images(tiff_file.imread(files[i+(arr_length*2)],key=[frame_key]), block_size, offset_val)\n",
    "    row4_thresh[i] = threshold_images(tiff_file.imread(files[i+(arr_length*3)],key=[frame_key]), block_size, offset_val)\n",
    "    row5_thresh[i] = threshold_images(tiff_file.imread(files[i+(arr_length*4)],key=[frame_key]), block_size, offset_val)\n",
    "    row6_thresh[i] = threshold_images(tiff_file.imread(files[i+(arr_length*5)],key=[frame_key]), block_size, offset_val)\n",
    "    \n",
    "    r1_total_num, r1_mean_size, r1_median_size, r1_mode_size = cluster_analysis(size_lim, i, 1, row1_thresh[i], rows, cols)\n",
    "    r2_total_num, r2_mean_size, r2_median_size, r2_mode_size = cluster_analysis(size_lim, i, 2, row2_thresh[i], rows, cols)\n",
    "    r3_total_num, r3_mean_size, r3_median_size, r3_mode_size = cluster_analysis(size_lim, i, 3, row3_thresh[i], rows, cols)\n",
    "    r4_total_num, r4_mean_size, r4_median_size, r4_mode_size = cluster_analysis(size_lim, i, 4, row4_thresh[i], rows, cols)\n",
    "    r5_total_num, r5_mean_size, r5_median_size, r5_mode_size = cluster_analysis(size_lim, i, 5, row5_thresh[i], rows, cols)\n",
    "    r6_total_num, r6_mean_size, r6_median_size, r6_mode_size = cluster_analysis(size_lim, i, 6, row6_thresh[i], rows, cols)\n",
    "    \n",
    "    row1_results[i] = (r1_total_num, r1_mean_size, r1_median_size, r1_mode_size)\n",
    "    row2_results[i] = (r2_total_num, r2_mean_size, r2_median_size, r2_mode_size)\n",
    "    row3_results[i] = (r3_total_num, r3_mean_size, r3_median_size, r3_mode_size)\n",
    "    row4_results[i] = (r4_total_num, r4_mean_size, r4_median_size, r4_mode_size)\n",
    "    row5_results[i] = (r5_total_num, r5_mean_size, r5_median_size, r5_mode_size)\n",
    "    row6_results[i] = (r6_total_num, r6_mean_size, r6_median_size, r6_mode_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5025af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap_num = (arr_length*2) - 2\n",
    "fig_size = 9.5, 20   #20/1.618\n",
    "\n",
    "### set up empty arrays\n",
    "all_total_num = np.zeros((total_rows,len(row1_results)))\n",
    "avg_total_num = np.empty(len(row1_results))\n",
    "stderror_total_num = np.empty(len(row1_results))\n",
    "\n",
    "### find average values and std. error \n",
    "for i in range(len(row1_results)):\n",
    "    all_total_num[0,i] = row1_results[i][0]\n",
    "    all_total_num[1,i] = row2_results[i][0]\n",
    "    all_total_num[2,i] = row3_results[i][0]\n",
    "    all_total_num[3,i] = row4_results[i][0]\n",
    "    all_total_num[4,i] = row5_results[i][0]\n",
    "    all_total_num[5,i] = row6_results[i][0]\n",
    "avg_total_num = all_total_num.mean(axis=0)\n",
    "stderror_total_num = all_total_num.std(axis=0)/np.sqrt(6)\n",
    "\n",
    "### same process for the other output results\n",
    "all_mean_size = np.zeros((6,len(row1_results)))\n",
    "avg_mean_size = np.empty(len(row1_results))\n",
    "stderror_mean_size = np.empty(len(row1_results))\n",
    "for i in range(len(row1_results)):\n",
    "    all_mean_size[0,i] = row1_results[i][1]\n",
    "    all_mean_size[1,i] = row2_results[i][1]\n",
    "    all_mean_size[2,i] = row3_results[i][1]\n",
    "    all_mean_size[3,i] = row4_results[i][1]\n",
    "    all_mean_size[4,i] = row5_results[i][1]\n",
    "    all_mean_size[5,i] = row6_results[i][1]\n",
    "avg_mean_size = all_mean_size.mean(axis=0)\n",
    "stderror_mean_size = all_mean_size.std(axis=0)/np.sqrt(6)\n",
    "\n",
    "all_median_size = np.zeros((6,len(row1_results)))\n",
    "avg_median_size = np.empty(len(row1_results))\n",
    "stderror_median_size = np.empty(len(row1_results))\n",
    "for i in range(len(row1_results)):\n",
    "    all_median_size[0,i] = row1_results[i][2]\n",
    "    all_median_size[1,i] = row2_results[i][2]\n",
    "    all_median_size[2,i] = row3_results[i][2]\n",
    "    all_median_size[3,i] = row4_results[i][2]\n",
    "    all_median_size[4,i] = row5_results[i][2]\n",
    "    all_median_size[5,i] = row6_results[i][2]\n",
    "avg_median_size = all_median_size.mean(axis=0)\n",
    "stderror_median_size = all_median_size.std(axis=0)/np.sqrt(6)\n",
    "\n",
    "all_mode_size = np.zeros((6,len(row1_results)))\n",
    "avg_mode_size = np.empty(len(row1_results))\n",
    "stderror_mode_size = np.empty(len(row1_results))\n",
    "for i in range(len(row1_results)):\n",
    "    all_mode_size[0,i] = row1_results[i][3]\n",
    "    all_mode_size[1,i] = row2_results[i][3]\n",
    "    all_mode_size[2,i] = row3_results[i][3]\n",
    "    all_mode_size[3,i] = row4_results[i][3]\n",
    "    all_mode_size[4,i] = row5_results[i][3]\n",
    "    all_mode_size[5,i] = row6_results[i][3]\n",
    "avg_mode_size = all_mode_size.mean(axis=0)\n",
    "stderror_mode_size = all_mode_size.std(axis=0)/np.sqrt(6)\n",
    "\n",
    "### plot average values and std. error for L1 correlation lengths\n",
    "fig = plt.figure(figsize=(fig_size))\n",
    "gs = fig.add_gridspec(4, top=0.95, hspace=0.09)\n",
    "axs = gs.subplots(sharex=False, sharey=False)\n",
    "#fig, axs = plt.subplots(3, figsize=(fig_size))\n",
    "markerSize = 8\n",
    "title = condition +\" n results (clusters= at least \"+str(beads_per_cluster)+\" beads, filter= \"+fil+\")\"\n",
    "\n",
    "for i in range(int(len(time_array))):\n",
    "    axs[0].set_title(title, fontsize= (font_size))\n",
    "    axs[0].set( ylabel='clusters found (number)')\n",
    "    axs[0].set_ylim(0, 400)\n",
    "    axs[0].plot(time_array[i], avg_total_num[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"max cluster size\")\n",
    "    axs[0].errorbar(time_array[i], avg_total_num[i], yerr = stderror_total_num[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "    axs[1].set( ylabel='mean cluster size ($\\mu$m)^2')\n",
    "    axs[1].set_ylim(0, 60)\n",
    "    axs[1].plot(time_array[i], avg_mean_size[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"mean cluster size\")\n",
    "    axs[1].errorbar(time_array[i], avg_mean_size[i], yerr = stderror_mean_size[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "    axs[2].set( ylabel='median cluster size ($\\mu$m)^2')\n",
    "    axs[2].set_ylim(0, 60)\n",
    "    axs[2].plot(time_array[i], avg_median_size[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"mean cluster size\")\n",
    "    axs[2].errorbar(time_array[i], avg_median_size[i], yerr = stderror_median_size[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "    axs[3].set( ylabel='mode cluster size ($\\mu$m)^2')\n",
    "    axs[3].set( xlabel=\"Time (hrs after adding KaiC)\")\n",
    "    axs[3].set_ylim(0, 30)\n",
    "    axs[3].plot(time_array[i], avg_mode_size[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)), label = \"mean cluster size\")\n",
    "    axs[3].errorbar(time_array[i], avg_mode_size[i], yerr = stderror_mode_size[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(axis='both', which='major', labelsize=(font_size-3))\n",
    "    ax.xaxis.get_label().set_fontsize(font_size)\n",
    "    ax.yaxis.get_label().set_fontsize(font_size)\n",
    "\n",
    "#plt.subplot_tool()\n",
    "plt.show()\n",
    "fig.savefig(saveto_dir+title+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_file = exp+\"-- results (clusters= at least \"+str(beads_per_cluster)+\" beads, filter= \"+fil+\")\"+\".csv\"\n",
    "data_file_exists = os.path.isfile(saveto_dir+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(\"Data file already exists.\")\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title])\n",
    "        writer.writerow(['clusters detected','','','','','','','','','',\n",
    "                         'mean cluster size','','','','','','','','','',\n",
    "                         'median cluster size','','','','','','','','',\n",
    "                         'mode cluster size',''])\n",
    "        writer.writerow(['time (hrs)',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i], \n",
    "                             row1_results[i][0], row2_results[i][0], row3_results[i][0], row4_results[i][0],\n",
    "                             row5_results[i][0], row6_results[i][0], avg_total_num[i], stderror_total_num[i], '',\n",
    "                             row1_results[i][1], row2_results[i][1], row3_results[i][1], row4_results[i][1], \n",
    "                             row5_results[i][1], row6_results[i][1], avg_mean_size[i], stderror_mean_size[i], '',\n",
    "                             row1_results[i][2], row2_results[i][2], row3_results[i][2], row4_results[i][2], \n",
    "                             row5_results[i][2], row6_results[i][2], avg_median_size[i], stderror_median_size[i], '', \n",
    "                             row1_results[i][3], row2_results[i][3], row3_results[i][3], row4_results[i][3], \n",
    "                             row5_results[i][3], row6_results[i][3], avg_mode_size[i], stderror_mode_size[i],'',])\n",
    "        writer.writerow([''])\n",
    "        \n",
    "    f.close()\n",
    "    print(\"Results appended to file.\")\n",
    "else:\n",
    "    print(\"Data file does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(saveto_dir+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([title])\n",
    "        writer.writerow(['clusters detected','','','','','','','','','',\n",
    "                         'mean cluster size','','','','','','','','','',\n",
    "                         'median cluster size','','','','','','','','',\n",
    "                         'mode cluster size',''])\n",
    "        writer.writerow(['time (hrs)',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',\n",
    "                         'row1','row2','row3','row4','row5','row6','avg','std error','',])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i], \n",
    "                             row1_results[i][0], row2_results[i][0], row3_results[i][0], row4_results[i][0],\n",
    "                             row5_results[i][0], row6_results[i][0], avg_total_num[i], stderror_total_num[i], '',\n",
    "                             row1_results[i][1], row2_results[i][1], row3_results[i][1], row4_results[i][1], \n",
    "                             row5_results[i][1], row6_results[i][1], avg_mean_size[i], stderror_mean_size[i], '',\n",
    "                             row1_results[i][2], row2_results[i][2], row3_results[i][2], row4_results[i][2], \n",
    "                             row5_results[i][2], row6_results[i][2], avg_median_size[i], stderror_median_size[i], '', \n",
    "                             row1_results[i][3], row2_results[i][3], row3_results[i][3], row4_results[i][3], \n",
    "                             row5_results[i][3], row6_results[i][3], avg_mode_size[i], stderror_mode_size[i],'',])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc319f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_data_file = exp+\"--BP results (clusters= at least \"+str(beads_per_cluster)+\" beads, filter= \"+fil+\")\"+\".csv\"\n",
    "data_file_exists = os.path.isfile(saveto_dir+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(\"Data file already exists.\")\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['condition:','',title])\n",
    "        writer.writerow(['clusters detected','','','','','','','','','','','',\n",
    "                         'mean cluster size','','','','','','','','','','','',\n",
    "                         'median cluster size','','','','','','','','','','','',\n",
    "                         'mode cluster size','','','','','','','','','','','',])\n",
    "        writer.writerow(['time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',])\n",
    "        \n",
    "        writer.writerow(['row1',row1_results[1][0],row1_results[2][0],row1_results[3][0],row1_results[4][0],row1_results[5][0],\n",
    "                         row1_results[6][0],row1_results[7][0],row1_results[8][0],row1_results[i][0],'','',\n",
    "                         'row1',row1_results[1][1],row1_results[2][1],row1_results[3][1],row1_results[4][1],row1_results[5][1],\n",
    "                         row1_results[6][1],row1_results[7][1],row1_results[8][1],row1_results[i][1],'','',\n",
    "                         'row1',row1_results[1][2],row1_results[2][2],row1_results[3][2],row1_results[4][2],row1_results[5][2],\n",
    "                         row1_results[6][2],row1_results[7][2],row1_results[8][2],row1_results[i][2],'','',\n",
    "                         'row1',row1_results[1][3],row1_results[2][3],row1_results[3][3],row1_results[4][3],row1_results[5][3],\n",
    "                         row1_results[6][3],row1_results[7][3],row1_results[8][3],row1_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row2',row2_results[1][0],row2_results[2][0],row2_results[3][0],row2_results[4][0],row2_results[5][0],\n",
    "                         row2_results[6][0],row2_results[7][0],row2_results[8][0],row2_results[i][0],'','',\n",
    "                         'row2',row2_results[1][1],row2_results[2][1],row2_results[3][1],row2_results[4][1],row2_results[5][1],\n",
    "                         row2_results[6][1],row2_results[7][1],row2_results[8][1],row2_results[i][1],'','',\n",
    "                         'row2',row2_results[1][2],row2_results[2][2],row2_results[3][2],row2_results[4][2],row2_results[5][2],\n",
    "                         row2_results[6][2],row2_results[7][2],row2_results[8][2],row2_results[i][2],'','',\n",
    "                         'row2',row2_results[1][3],row2_results[2][3],row2_results[3][3],row2_results[4][3],row2_results[5][3],\n",
    "                         row2_results[6][3],row2_results[7][3],row2_results[8][3],row2_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row3',row3_results[1][0],row3_results[2][0],row3_results[3][0],row3_results[4][0],row3_results[5][0],\n",
    "                         row3_results[6][0],row3_results[7][0],row3_results[8][0],row3_results[i][0],'','',\n",
    "                         'row3',row3_results[1][1],row3_results[2][1],row3_results[3][1],row3_results[4][1],row3_results[5][1],\n",
    "                         row3_results[6][1],row3_results[7][1],row3_results[8][1],row3_results[i][1],'','',\n",
    "                         'row3',row3_results[1][2],row3_results[2][2],row3_results[3][2],row3_results[4][2],row3_results[5][2],\n",
    "                         row3_results[6][2],row3_results[7][2],row3_results[8][2],row3_results[i][2],'','',\n",
    "                         'row3',row3_results[1][3],row3_results[2][3],row3_results[3][3],row3_results[4][3],row3_results[5][3],\n",
    "                         row3_results[6][3],row3_results[7][3],row3_results[8][3],row3_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row4',row4_results[1][0],row4_results[2][0],row4_results[3][0],row4_results[4][0],row4_results[5][0],\n",
    "                         row4_results[6][0],row4_results[7][0],row4_results[8][0],row4_results[i][0],'','',\n",
    "                         'row4',row4_results[1][1],row4_results[2][1],row4_results[3][1],row4_results[4][1],row4_results[5][1],\n",
    "                         row4_results[6][1],row4_results[7][1],row4_results[8][1],row4_results[i][1],'','',\n",
    "                         'row4',row4_results[1][2],row4_results[2][2],row4_results[3][2],row4_results[4][2],row4_results[5][2],\n",
    "                         row4_results[6][2],row4_results[7][2],row4_results[8][2],row4_results[i][2],'','',\n",
    "                         'row4',row4_results[1][3],row4_results[2][3],row4_results[3][3],row4_results[4][3],row4_results[5][3],\n",
    "                         row4_results[6][3],row4_results[7][3],row4_results[8][3],row4_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row5',row5_results[1][0],row5_results[2][0],row5_results[3][0],row5_results[4][0],row5_results[5][0],\n",
    "                         row5_results[6][0],row5_results[7][0],row5_results[8][0],row5_results[i][0],'','',\n",
    "                         'row5',row5_results[1][1],row5_results[2][1],row5_results[3][1],row5_results[4][1],row5_results[5][1],\n",
    "                         row5_results[6][1],row5_results[7][1],row5_results[8][1],row5_results[i][1],'','',\n",
    "                         'row5',row5_results[1][2],row5_results[2][2],row5_results[3][2],row5_results[4][2],row5_results[5][2],\n",
    "                         row5_results[6][2],row5_results[7][2],row5_results[8][2],row5_results[i][2],'','',\n",
    "                         'row5',row5_results[1][3],row5_results[2][3],row5_results[3][3],row5_results[4][3],row5_results[5][3],\n",
    "                         row5_results[6][3],row5_results[7][3],row5_results[8][3],row5_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row6',row6_results[1][0],row6_results[2][0],row6_results[3][0],row6_results[4][0],row6_results[5][0],\n",
    "                         row6_results[6][0],row6_results[7][0],row6_results[8][0],row6_results[i][0],'','',\n",
    "                         'row6',row6_results[1][1],row6_results[2][1],row6_results[3][1],row6_results[4][1],row6_results[5][1],\n",
    "                         row6_results[6][1],row6_results[7][1],row6_results[8][1],row6_results[i][1],'','',\n",
    "                         'row6',row6_results[1][2],row6_results[2][2],row6_results[3][2],row6_results[4][2],row6_results[5][2],\n",
    "                         row6_results[6][2],row6_results[7][2],row6_results[8][2],row6_results[i][2],'','',\n",
    "                         'row6',row6_results[1][3],row6_results[2][3],row6_results[3][3],row6_results[4][3],row6_results[5][3],\n",
    "                         row6_results[6][3],row6_results[7][3],row6_results[8][3],row6_results[i][3],'','',])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"Results appended to file.\")\n",
    "else:\n",
    "    print(\"Data file does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(saveto_dir+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['condition:','',title])\n",
    "        writer.writerow(['clusters detected','','','','','','','','','','','','',\n",
    "                         'mean cluster size','','','','','','','','','','','',\n",
    "                         'median cluster size','','','','','','','','','','','',\n",
    "                         'mode cluster size','','','','','','','','','','','',])\n",
    "        writer.writerow(['time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',\n",
    "                         'time (hrs)',time_array[0],time_array[1],time_array[2],time_array[3],time_array[4],time_array[5],\n",
    "                         time_array[6],time_array[7],time_array[8],'','',])\n",
    "        \n",
    "        writer.writerow(['row1',row1_results[1][0],row1_results[2][0],row1_results[3][0],row1_results[4][0],row1_results[5][0],\n",
    "                         row1_results[6][0],row1_results[7][0],row1_results[8][0],row1_results[i][0],'','',\n",
    "                         'row1',row1_results[1][1],row1_results[2][1],row1_results[3][1],row1_results[4][1],row1_results[5][1],\n",
    "                         row1_results[6][1],row1_results[7][1],row1_results[8][1],row1_results[i][1],'','',\n",
    "                         'row1',row1_results[1][2],row1_results[2][2],row1_results[3][2],row1_results[4][2],row1_results[5][2],\n",
    "                         row1_results[6][2],row1_results[7][2],row1_results[8][2],row1_results[i][2],'','',\n",
    "                         'row1',row1_results[1][3],row1_results[2][3],row1_results[3][3],row1_results[4][3],row1_results[5][3],\n",
    "                         row1_results[6][3],row1_results[7][3],row1_results[8][3],row1_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row2',row2_results[1][0],row2_results[2][0],row2_results[3][0],row2_results[4][0],row2_results[5][0],\n",
    "                         row2_results[6][0],row2_results[7][0],row2_results[8][0],row2_results[i][0],'','',\n",
    "                         'row2',row2_results[1][1],row2_results[2][1],row2_results[3][1],row2_results[4][1],row2_results[5][1],\n",
    "                         row2_results[6][1],row2_results[7][1],row2_results[8][1],row2_results[i][1],'','',\n",
    "                         'row2',row2_results[1][2],row2_results[2][2],row2_results[3][2],row2_results[4][2],row2_results[5][2],\n",
    "                         row2_results[6][2],row2_results[7][2],row2_results[8][2],row2_results[i][2],'','',\n",
    "                         'row2',row2_results[1][3],row2_results[2][3],row2_results[3][3],row2_results[4][3],row2_results[5][3],\n",
    "                         row2_results[6][3],row2_results[7][3],row2_results[8][3],row2_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row3',row3_results[1][0],row3_results[2][0],row3_results[3][0],row3_results[4][0],row3_results[5][0],\n",
    "                         row3_results[6][0],row3_results[7][0],row3_results[8][0],row3_results[i][0],'','',\n",
    "                         'row3',row3_results[1][1],row3_results[2][1],row3_results[3][1],row3_results[4][1],row3_results[5][1],\n",
    "                         row3_results[6][1],row3_results[7][1],row3_results[8][1],row3_results[i][1],'','',\n",
    "                         'row3',row3_results[1][2],row3_results[2][2],row3_results[3][2],row3_results[4][2],row3_results[5][2],\n",
    "                         row3_results[6][2],row3_results[7][2],row3_results[8][2],row3_results[i][2],'','',\n",
    "                         'row3',row3_results[1][3],row3_results[2][3],row3_results[3][3],row3_results[4][3],row3_results[5][3],\n",
    "                         row3_results[6][3],row3_results[7][3],row3_results[8][3],row3_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row4',row4_results[1][0],row4_results[2][0],row4_results[3][0],row4_results[4][0],row4_results[5][0],\n",
    "                         row4_results[6][0],row4_results[7][0],row4_results[8][0],row4_results[i][0],'','',\n",
    "                         'row4',row4_results[1][1],row4_results[2][1],row4_results[3][1],row4_results[4][1],row4_results[5][1],\n",
    "                         row4_results[6][1],row4_results[7][1],row4_results[8][1],row4_results[i][1],'','',\n",
    "                         'row4',row4_results[1][2],row4_results[2][2],row4_results[3][2],row4_results[4][2],row4_results[5][2],\n",
    "                         row4_results[6][2],row4_results[7][2],row4_results[8][2],row4_results[i][2],'','',\n",
    "                         'row4',row4_results[1][3],row4_results[2][3],row4_results[3][3],row4_results[4][3],row4_results[5][3],\n",
    "                         row4_results[6][3],row4_results[7][3],row4_results[8][3],row4_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row5',row5_results[1][0],row5_results[2][0],row5_results[3][0],row5_results[4][0],row5_results[5][0],\n",
    "                         row5_results[6][0],row5_results[7][0],row5_results[8][0],row5_results[i][0],'','',\n",
    "                         'row5',row5_results[1][1],row5_results[2][1],row5_results[3][1],row5_results[4][1],row5_results[5][1],\n",
    "                         row5_results[6][1],row5_results[7][1],row5_results[8][1],row5_results[i][1],'','',\n",
    "                         'row5',row5_results[1][2],row5_results[2][2],row5_results[3][2],row5_results[4][2],row5_results[5][2],\n",
    "                         row5_results[6][2],row5_results[7][2],row5_results[8][2],row5_results[i][2],'','',\n",
    "                         'row5',row5_results[1][3],row5_results[2][3],row5_results[3][3],row5_results[4][3],row5_results[5][3],\n",
    "                         row5_results[6][3],row5_results[7][3],row5_results[8][3],row5_results[i][3],'','',])\n",
    "        \n",
    "        writer.writerow(['row6',row6_results[1][0],row6_results[2][0],row6_results[3][0],row6_results[4][0],row6_results[5][0],\n",
    "                         row6_results[6][0],row6_results[7][0],row6_results[8][0],row6_results[i][0],'','',\n",
    "                         'row6',row6_results[1][1],row6_results[2][1],row6_results[3][1],row6_results[4][1],row6_results[5][1],\n",
    "                         row6_results[6][1],row6_results[7][1],row6_results[8][1],row6_results[i][1],'','',\n",
    "                         'row6',row6_results[1][2],row6_results[2][2],row6_results[3][2],row6_results[4][2],row6_results[5][2],\n",
    "                         row6_results[6][2],row6_results[7][2],row6_results[8][2],row6_results[i][2],'','',\n",
    "                         'row6',row6_results[1][3],row6_results[2][3],row6_results[3][3],row6_results[4][3],row6_results[5][3],\n",
    "                         row6_results[6][3],row6_results[7][3],row6_results[8][3],row6_results[i][3],'','',])\n",
    "        writer.writerow([''])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4f520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c10c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
